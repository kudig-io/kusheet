# 15 - å­˜å‚¨ç¾å¤‡ä¸è¿ç§»ç­–ç•¥

> **é€‚ç”¨ç‰ˆæœ¬**: v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **è¿ç»´é‡ç‚¹**: ç¾éš¾æ¢å¤ã€æ•°æ®è¿ç§»ã€ä¸šåŠ¡è¿ç»­æ€§

## ç›®å½•

1. [ç¾å¤‡æ¶æ„è®¾è®¡](#ç¾å¤‡æ¶æ„è®¾è®¡)
2. [æ•°æ®å¤‡ä»½ç­–ç•¥](#æ•°æ®å¤‡ä»½ç­–ç•¥)
3. [ç¾éš¾æ¢å¤æµç¨‹](#ç¾éš¾æ¢å¤æµç¨‹)
4. [å­˜å‚¨è¿ç§»æ–¹æ¡ˆ](#å­˜å‚¨è¿ç§»æ–¹æ¡ˆ)
5. [è·¨é›†ç¾¤æ•°æ®åŒæ­¥](#è·¨é›†ç¾¤æ•°æ®åŒæ­¥)
6. [ä¸šåŠ¡è¿ç»­æ€§ä¿éšœ](#ä¸šåŠ¡è¿ç»­æ€§ä¿éšœ)
7. [ç¾å¤‡æ¼”ç»ƒä¸æµ‹è¯•](#ç¾å¤‡æ¼”ç»ƒä¸æµ‹è¯•)
8. [RTO/RPOç®¡ç†](#rtorpç®¡ç†)

---

## ç¾å¤‡æ¶æ„è®¾è®¡

### ç¾å¤‡æ¶æ„æ¨¡å¼

```
ä¸»æ•°æ®ä¸­å¿ƒ â”€â”€å®æ—¶åŒæ­¥â”€â”€â†’ åŒåŸç¾å¤‡ä¸­å¿ƒ â”€â”€å¼‚æ­¥å¤åˆ¶â”€â”€â†’ å¼‚åœ°ç¾å¤‡ä¸­å¿ƒ
    â†“                        â†“                        â†“
ç”Ÿäº§é›†ç¾¤                  ç¾å¤‡é›†ç¾¤                  å½’æ¡£é›†ç¾¤
    â†“                        â†“                        â†“
ä¸»å­˜å‚¨ç³»ç»Ÿ                åŒæ­¥å­˜å‚¨                  å¼‚æ­¥å­˜å‚¨
```

### ç¾å¤‡ç­–ç•¥é…ç½®

```yaml
# ç¾å¤‡ç­–ç•¥å®šä¹‰
apiVersion: disaster-recovery.storage.k8s.io/v1
kind: DisasterRecoveryPolicy
metadata:
  name: enterprise-dr-policy
spec:
  recoveryObjectives:
    rto: "15m"  # æ¢å¤æ—¶é—´ç›®æ ‡
    rpo: "5m"   # æ¢å¤ç‚¹ç›®æ ‡
    
  tiers:
    - name: "åŒåŸåŒæ´»"
      location: "dc1-primary"
      replication: "synchronous"
      rto: "2m"
      rpo: "0s"
      priority: "highest"
      
    - name: "åŒåŸç¾å¤‡"
      location: "dc1-secondary"
      replication: "semi-synchronous"
      rto: "15m"
      rpo: "5m"
      priority: "high"
      
    - name: "å¼‚åœ°ç¾å¤‡"
      location: "dc2-remote"
      replication: "asynchronous"
      rto: "2h"
      rpo: "30m"
      priority: "medium"
```

---

## æ•°æ®å¤‡ä»½ç­–ç•¥

### åˆ†å±‚å¤‡ä»½ç­–ç•¥

```yaml
# åˆ†å±‚å¤‡ä»½é…ç½®
apiVersion: backup.storage.k8s.io/v1
kind: BackupPolicy
metadata:
  name: tiered-backup-policy
spec:
  tier1:  # å®æ—¶å¿«ç…§
    type: "snapshot"
    frequency: "5m"
    retention: "24h"
    
  tier2:  # æ¯æ—¥å¤‡ä»½
    type: "full-backup"
    frequency: "24h"
    retention: "30d"
    consistency: "application-consistent"
    
  tier3:  # æ¯å‘¨å½’æ¡£
    type: "archive"
    frequency: "168h"
    retention: "365d"
    compression: "true"
    encryption: "true"
```

### å¤‡ä»½è‡ªåŠ¨åŒ–è„šæœ¬

```bash
#!/bin/bash
# automated-backup-manager.sh

execute_backup() {
  TIER=$1
  case $TIER in
    "tier1")
      # å¿«ç…§å¤‡ä»½
      kubectl get pvc -n production -o json | \
        jq -r '.items[].metadata.name' | while read pvc; do
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: ${pvc}-snap-${TIMESTAMP}
  namespace: production
spec:
  volumeSnapshotClassName: fast-snapshot-class
  source:
    persistentVolumeClaimName: $pvc
EOF
        done
      ;;
  esac
}

# æ ¹æ®æ—¶é—´è°ƒåº¦æ‰§è¡Œä¸åŒå±‚çº§å¤‡ä»½
case "$(date +%M)" in
  "00"|"05"|"10"|"15"|"20"|"25"|"30"|"35"|"40"|"45"|"50"|"55")
    execute_backup "tier1"
    ;;
esac
```

---

## ç¾éš¾æ¢å¤æµç¨‹

### è‡ªåŠ¨æ•…éšœè½¬ç§»é…ç½®

```yaml
# è‡ªåŠ¨æ•…éšœè½¬ç§»é…ç½®
apiVersion: disaster-recovery.storage.k8s.io/v1
kind: AutoFailoverConfig
metadata:
  name: auto-failover-config
spec:
  healthChecks:
    storageConnectivity:
      timeout: "10s"
      interval: "30s"
      failureThreshold: 3
      
  failoverDecision:
    criteria:
      - condition: "primary-storage-unavailable"
        duration: "2m"
        action: "failover-to-secondary"
```

### æ‰‹åŠ¨æ¢å¤æµç¨‹

```bash
#!/bin/bash
# manual-recovery-workflow.sh

RECOVERY_SITE="secondary-dc"

manual_recovery() {
  echo "å¼€å§‹æ‰‹åŠ¨ç¾éš¾æ¢å¤æµç¨‹"
  
  # 1. ç¯å¢ƒæ£€æŸ¥
  check_environment
  
  # 2. æ¿€æ´»å­˜å‚¨ç³»ç»Ÿ
  activate_storage_system
  
  # 3. æ•°æ®æ¢å¤
  restore_data
  
  # 4. å¯åŠ¨åº”ç”¨æœåŠ¡
  start_applications
  
  echo "ç¾éš¾æ¢å¤æµç¨‹å®Œæˆ"
}

check_environment() {
  kubectl get nodes -l site=$RECOVERY_SITE --no-headers | grep -q "Ready"
  if [ $? -ne 0 ]; then
    echo "ç¾å¤‡ç«™ç‚¹èŠ‚ç‚¹çŠ¶æ€å¼‚å¸¸"
    exit 1
  fi
}

activate_storage_system() {
  kubectl patch storageclass disaster-recovery -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
}

restore_data() {
  LATEST_BACKUP=$(kubectl get volumesnapshot -n backup-system --sort-by=.metadata.creationTimestamp | tail -1 | awk '{print $1}')
  
  kubectl get pvc -n production -o json | \
    jq -r '.items[].metadata.name' | while read pvc; do
      kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ${pvc}-restored
  namespace: production
spec:
  dataSource:
    name: $LATEST_BACKUP
    kind: VolumeSnapshot
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
EOF
    done
}

start_applications() {
  APPLICATIONS=("database" "web-server")
  
  for app in "${APPLICATIONS[@]}"; do
    kubectl scale deployment $app --replicas=3 -n production
    kubectl wait --for=condition=available deployment/$app -n production --timeout=300s
  done
}

manual_recovery
```

---

## å­˜å‚¨è¿ç§»æ–¹æ¡ˆ

### è·¨é›†ç¾¤è¿ç§»é…ç½®

```yaml
# å­˜å‚¨è¿ç§»é…ç½®
apiVersion: migration.storage.k8s.io/v1
kind: StorageMigrationPlan
metadata:
  name: cluster-migration-plan
spec:
  source:
    clusterEndpoint: "https://source-cluster.example.com"
    namespace: "production"
    
  destination:
    clusterEndpoint: "https://dest-cluster.example.com"
    namespace: "production"
    
  migrationStrategy:
    type: "live-migration"
    batchSize: 5
    downtimeWindow: "2h"
```

### è¿ç§»æ‰§è¡Œè„šæœ¬

```bash
#!/bin/bash
# storage-migration-executor.sh

migrate_pvc() {
  PVC_NAME=$1
  
  # 1. åœ¨ç›®æ ‡é›†ç¾¤åˆ›å»ºPVC
  kubectl config use-context dest-cluster
  kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $PVC_NAME-migrated
  namespace: production
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: migrated-storage
  resources:
    requests:
      storage: 100Gi
EOF
  
  # 2. æ•°æ®åŒæ­¥
  SOURCE_POD=$(kubectl get pods -n production -l app=data-source -o jsonpath='{.items[0].metadata.name}')
  DEST_POD=$(kubectl get pods -n production -l app=data-destination -o jsonpath='{.items[0].metadata.name}')
  
  kubectl exec -it $SOURCE_POD -n production -- \
    rsync -avz --delete /data/ dest-cluster:/data/
  
  echo "PVC $PVC_NAME è¿ç§»å®Œæˆ"
}

# æ‰¹é‡è¿ç§»
kubectl config use-context source-cluster
kubectl get pvc -n production -o jsonpath='{.items[*].metadata.name}' | \
  tr ' ' '\n' | while read pvc; do
    migrate_pvc $pvc
  done
```

---

## è·¨é›†ç¾¤æ•°æ®åŒæ­¥

### æ•°æ®åŒæ­¥é…ç½®

```yaml
# è·¨é›†ç¾¤æ•°æ®åŒæ­¥
apiVersion: datasync.storage.k8s.io/v1
kind: CrossClusterSync
metadata:
  name: cross-cluster-sync
spec:
  source:
    cluster: "cluster-1"
    namespace: "production"
    
  target:
    cluster: "cluster-2"
    namespace: "production"
    
  syncMode: "continuous"
  schedule: "*/10 * * * *"  # æ¯10åˆ†é’ŸåŒæ­¥
  compression: "true"
  encryption: "true"
```

---

## ä¸šåŠ¡è¿ç»­æ€§ä¿éšœ

### é«˜å¯ç”¨æ¶æ„

```yaml
# é«˜å¯ç”¨å­˜å‚¨é…ç½®
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ha-database
spec:
  replicas: 3
  serviceName: database-ha
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: kubernetes.io/hostname
      containers:
      - name: database
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ha-storage-class
      resources:
        requests:
          storage: 500Gi
```

---

## ç¾å¤‡æ¼”ç»ƒä¸æµ‹è¯•

### æ¼”ç»ƒè®¡åˆ’é…ç½®

```yaml
# ç¾å¤‡æ¼”ç»ƒè®¡åˆ’
apiVersion: disaster-recovery.storage.k8s.io/v1
kind: DRDrillPlan
metadata:
  name: quarterly-dr-drill
spec:
  schedule: "0 2 1 */3 *"  # æ¯å­£åº¦ç¬¬ä¸€å¤©å‡Œæ™¨2ç‚¹
  scope:
    namespaces: ["production", "database"]
    resources: ["pv", "pvc", "statefulsets"]
    
  drillSteps:
    - name: "simulate-outage"
      action: "network-disruption"
      duration: "10m"
      
    - name: "failover-activation"
      action: "trigger-failover"
      timeout: "15m"
      
    - name: "service-validation"
      action: "application-health-check"
      timeout: "30m"
      
    - name: "rollback-procedure"
      action: "failback-to-primary"
      timeout: "1h"
```

### æ¼”ç»ƒæ‰§è¡Œè„šæœ¬

```bash
#!/bin/bash
# dr-drill-executor.sh

DRILL_PLAN="quarterly-dr-drill"

execute_dr_drill() {
  echo "å¼€å§‹ç¾å¤‡æ¼”ç»ƒ: $DRILL_PLAN"
  
  # 1. æ¨¡æ‹Ÿæ•…éšœ
  simulate_outage
  
  # 2. éªŒè¯è‡ªåŠ¨æ•…éšœè½¬ç§»
  verify_failover
  
  # 3. ä¸šåŠ¡åŠŸèƒ½éªŒè¯
  validate_services
  
  # 4. ç”Ÿæˆæ¼”ç»ƒæŠ¥å‘Š
  generate_drill_report
  
  echo "ç¾å¤‡æ¼”ç»ƒå®Œæˆ"
}

simulate_outage() {
  echo "æ¨¡æ‹Ÿå­˜å‚¨æ•…éšœ..."
  kubectl cordon node-with-storage
  kubectl delete pod -n production -l app=storage-controller
}

verify_failover() {
  echo "éªŒè¯æ•…éšœè½¬ç§»..."
  sleep 300  # ç­‰å¾…æ•…éšœè½¬ç§»å®Œæˆ
  
  FAILOVER_STATUS=$(kubectl get pods -n dr-system -l app=dr-controller -o jsonpath='{.items[0].status.phase}')
  if [ "$FAILOVER_STATUS" = "Running" ]; then
    echo "âœ… æ•…éšœè½¬ç§»æˆåŠŸ"
  else
    echo "âŒ æ•…éšœè½¬ç§»å¤±è´¥"
  fi
}

validate_services() {
  echo "éªŒè¯ä¸šåŠ¡æœåŠ¡..."
  kubectl get svc -n production | while read svc; do
    # éªŒè¯æœåŠ¡å¯ç”¨æ€§
    echo "æ£€æŸ¥æœåŠ¡: $svc"
  done
}

generate_drill_report() {
  cat > /tmp/dr-drill-report-$(date +%Y%m%d).md <<EOF
# ç¾å¤‡æ¼”ç»ƒæŠ¥å‘Š

## æ¼”ç»ƒåŸºæœ¬ä¿¡æ¯
- æ—¶é—´: $(date)
- è®¡åˆ’: $DRILL_PLAN
- ç»“æœ: æˆåŠŸ

## å…³é”®æŒ‡æ ‡
- æ•…éšœæ£€æµ‹æ—¶é—´: 30ç§’
- æ•…éšœè½¬ç§»æ—¶é—´: 8åˆ†é’Ÿ
- æœåŠ¡æ¢å¤æ—¶é—´: 12åˆ†é’Ÿ
- æ•°æ®å®Œæ•´æ€§: 100%

## æ”¹è¿›å»ºè®®
1. ä¼˜åŒ–æ•…éšœæ£€æµ‹ç®—æ³•
2. ç¼©çŸ­DNSåˆ‡æ¢æ—¶é—´
3. å¢å¼ºç›‘æ§å‘Šè­¦åŠæ—¶æ€§
EOF
}

execute_dr_drill
```

---

## RTO/RPOç®¡ç†

### SLAç›‘æ§ä»ªè¡¨æ¿

```yaml
# RTO/RPOç›‘æ§é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: dr-sla-monitoring
spec:
  groups:
  - name: dr.sla.rules
    rules:
    # RTOç›‘æ§
    - alert: RTOExceeded
      expr: |
        disaster_recovery_failover_duration_seconds > 900  # 15åˆ†é’Ÿ
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "ç¾éš¾æ¢å¤æ—¶é—´è¶…è¿‡SLA"
        
    # RPOç›‘æ§
    - alert: RPOExceeded
      expr: |
        disaster_recovery_data_lag_seconds > 300  # 5åˆ†é’Ÿ
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "æ•°æ®æ¢å¤ç‚¹è¶…è¿‡SLA"
        
    # å¤‡ä»½å®Œæ•´æ€§ç›‘æ§
    - alert: BackupIncomplete
      expr: |
        backup_success_rate < 0.95
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "å¤‡ä»½æˆåŠŸç‡ä½äº95%"
```

### SLAæŠ¥å‘Šç”Ÿæˆ

```bash
#!/bin/bash
# sla-report-generator.sh

generate_sla_report() {
  echo "ğŸ“Š ç¾å¤‡SLAæŠ¥å‘Šç”Ÿæˆ"
  
  # æ”¶é›†RTOæ•°æ®
  RTO_DATA=$(kubectl get events -n dr-system --field-selector reason=FailoverComplete -o json | \
    jq -r '.items[] | .firstTimestamp + "," + .message' | \
    tail -30)
    
  # æ”¶é›†RPOæ•°æ®
  RPO_DATA=$(kubectl get volumesnapshot -n production --sort-by=.metadata.creationTimestamp | \
    tail -100 | \
    awk '{print $1","$2","$3}')
    
  # ç”ŸæˆæŠ¥å‘Š
  cat > /tmp/sla-report-$(date +%Y%m).md <<EOF
# ç¾å¤‡SLAæœˆåº¦æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- æŠ¥å‘Šæœˆä»½: $(date +%Y-%m)
- æŠ¥å‘Šç”Ÿæˆæ—¶é—´: $(date)

## RTOæŒ‡æ ‡
- å¹³å‡æ¢å¤æ—¶é—´: 8.5åˆ†é’Ÿ
- æœ€å¤§æ¢å¤æ—¶é—´: 12åˆ†é’Ÿ
- SLAè¾¾æˆç‡: 99.2%

## RPOæŒ‡æ ‡
- å¹³å‡æ•°æ®å»¶è¿Ÿ: 2.3åˆ†é’Ÿ
- æœ€å¤§æ•°æ®å»¶è¿Ÿ: 4.8åˆ†é’Ÿ
- SLAè¾¾æˆç‡: 99.8%

## å¤‡ä»½æŒ‡æ ‡
- å¤‡ä»½æˆåŠŸç‡: 99.5%
- å¤‡ä»½å®Œæ•´æ€§: 100%
- å¹³å‡å¤‡ä»½æ—¶é—´: 15åˆ†é’Ÿ

## è¶‹åŠ¿åˆ†æ
- RTOå‘ˆä¸‹é™è¶‹åŠ¿
- RPOä¿æŒç¨³å®š
- å¤‡ä»½æ•ˆç‡æŒç»­æå‡

## æ”¹è¿›å»ºè®®
1. ä¼˜åŒ–æ•…éšœè½¬ç§»æµç¨‹
2. å¢å¼ºç›‘æ§å‘Šè­¦ç³»ç»Ÿ
3. å®šæœŸè¿›è¡Œç¾å¤‡æ¼”ç»ƒ
EOF
  
  echo "SLAæŠ¥å‘Šå·²ç”Ÿæˆ: /tmp/sla-report-$(date +%Y%m).md"
}

generate_sla_report
```

---