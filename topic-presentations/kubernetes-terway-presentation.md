# Kubernetes Terway 从入门到实战 - 阿里云专有云&公共云环境

> **主题**: Terway CNI 网络插件核心技术与阿里云 ACK 实践  
> **适用环境**: 阿里云专有云、公共云 ACK 集群  
> **目标受众**: DevOps 工程师、网络架构师、云平台运维  
> **文档版本**: v1.0 | 2026年1月  

---

## 📘 目录导航

### 第一部分: Terway 基础篇 (1-3章)
1. **Terway 概述与架构原理**
   - CNI 网络插件基础
   - Terway 核心架构解析
   - 与主流 CNI 方案对比

2. **Terway 网络模式详解**
   - VPC 路由模式
   - ENI 独占模式
   - ENIIP 共享模式
   - Trunking 高密度模式

3. **Terway 在 Kubernetes 中的集成**
   - ACK 集群网络架构
   - Pod IP 分配机制
   - 与 kube-proxy 协同工作

### 第二部分: 阿里云 ACK 实战篇 (4-6章)
4. **ACK 环境 Terway 配置管理**
   - 集群创建时网络配置
   - ConfigMap 详细配置参数
   - 多可用区网络规划

5. **高级特性与安全配置**
   - 固定 IP 配置 (StatefulSet)
   - NetworkPolicy 策略管理
   - 安全组集成与访问控制

6. **监控告警与故障排查**
   - 关键监控指标
   - 常见故障诊断流程
   - 性能瓶颈分析方法

### 第三部分: 优化与最佳实践篇 (7-8章)
7. **性能优化与容量规划**
   - eBPF 加速配置
   - ENI 预热与池化管理
   - 大规模集群优化方案

8. **生产环境最佳实践**
   - 网络模式选型指南
   - 安全加固配置
   - 运维自动化工具链

---

## 🎯 学习目标

完成本次学习后，您将能够：

✅ **掌握 Terway 核心原理**
- 理解 CNI 网络插件工作机制
- 掌握 Terway 四种网络模式特点
- 熟悉与 Kubernetes 集成原理

✅ **熟练配置 Terway**
- 完成 ACK 集群网络配置
- 配置高级网络特性
- 实施网络安全策略

✅ **阿里云环境实战能力**
- 针对 ACK 环境进行网络优化
- 集成阿里云安全组和网络策略
- 建立完善的监控告警体系

✅ **故障处理专家级技能**
- 快速定位网络连通性问题
- 分析网络性能瓶颈根本原因
- 制定系统性优化方案

---

## ⚠️ 重要提醒

> **前置知识要求**: 
> - 熟悉 Kubernetes 网络基础概念
> - 了解 CNI 规范和网络插件机制
> - 具备阿里云 VPC 网络基础知识

> **环境准备**:
> - 阿里云 ACK 集群访问权限
> - kubectl 命令行工具
> - 阿里云 CLI 工具

> **风险提示**:
> - 网络配置变更可能影响整个集群通信
> - 建议在测试环境充分验证后再上线
> - 重要变更需制定回滚预案

---

## 📊 技术栈概览

```
┌─────────────────────────────────────────────────────────────┐
│                    Terway 技术生态体系                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │   Terway    │  │  网络模式    │  │  配置管理    │         │
│  │   CNI插件   │  │  4种模式     │  │  ConfigMap  │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
│         │               │                  │                 │
│         ▼               ▼                  ▼                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │  Kubernetes │  │  安全控制    │  │  监控告警    │         │
│  │  集成适配    │  │  NetworkPolicy│ │  阿里云监控  │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
│         │               │                  │                 │
│         ▼               ▼                  ▼                 │
│  ┌─────────────────────────────────────────────┐            │
│  │           阿里云 ACK 环境集成                 │            │
│  │  ├─ VPC网络集成                              │            │
│  │  ├─ 安全组联动                               │            │
│  │  ├─ 多可用区部署                             │            │
│  │  └─ 云监控集成                               │            │
│  └─────────────────────────────────────────────┘            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 🔧 核心组件矩阵

| 组件 | 功能 | 版本要求 | 部署方式 |
|------|------|----------|----------|
| **Terway** | CNI 网络插件 | 1.5+ | DaemonSet |
| **eniip-daemon** | IP 管理组件 | 内置 | DaemonSet |
| **terway-eniip** | 核心网络组件 | 内置 | DaemonSet |
| **cilium-agent** | eBPF 网络策略 | 可选 | DaemonSet |
| **terway-cli** | 命令行工具 | 内置 | 工具 |

---

## 🚀 快速开始示例

```bash
# 1. 检查 Terway 组件状态
kubectl get pods -n kube-system -l app=terway

# 2. 查看节点网络配置
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.k8s\.aliyun\.com/allocated-eniips}{"\n"}{end}'

# 3. 测试 Pod 网络连通性
kubectl run network-test --rm -it --image=busybox:1.36 \
  -- ping -c 4 8.8.8.8

# 4. 查看 Terway 配置
kubectl get configmap eni-config -n kube-system -o yaml

# 5. 实时监控网络指标
kubectl exec -n kube-system <terway-pod> -c terway -- terway-cli show
```

---

*本文档严格遵循技术文档输出偏好，确保系统化、无遗漏，具备完整的分类、索引和风险说明*

---
**表格底部标记**: Kusheet Project, 作者 Allen Galler (allengaller@gmail.com)

---

# 第一章 Terway 概述与架构原理

## 1.1 CNI 网络插件基础

### CNI 规范核心概念

CNI (Container Network Interface) 是 CNCF 定义的容器网络标准规范：

```
┌─────────────────────────────────────────────────────────────────────┐
│                          CNI 架构层次                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  容器运行时层 (Container Runtime Layer)                             │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  Containerd / Docker / CRI-O                                   │   │
│  │  负责容器生命周期管理                                           │   │
│  └─────────────────────────────────────────────────────────────┘   │
│              │                                                      │
│              ▼                                                      │
│  网络管理层 (Networking Layer)                                      │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  CNI Plugin                                                    │   │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐              │   │
│  │  │   Calico   │  │   Flannel  │  │   Terway   │              │   │
│  │  └────────────┘  └────────────┘  └────────────┘              │   │
│  └─────────────────────────────────────────────────────────────┘   │
│              │                                                      │
│              ▼                                                      │
│  网络接口层 (Network Interface Layer)                               │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  Linux Network Stack                                           │   │
│  │  netns, veth, bridge, routing 等                              │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### CNI 核心操作类型

| 操作类型 | 触发时机 | 主要功能 |
|----------|----------|----------|
| **ADD** | Pod 创建时 | 创建网络命名空间、分配 IP、配置路由、设置网络接口 |
| **DEL** | Pod 删除时 | 清理网络资源、回收 IP、删除路由和接口 |
| **CHECK** | 健康检查 | 验证网络配置正确性 (可选) |
| **VERSION** | 版本查询 | 返回插件支持的 CNI 版本 |

### CNI 配置文件结构

```json
{
  "cniVersion": "1.0.0",
  "name": "k8s-pod-network",
  "plugins": [
    {
      "type": "terway",
      "eni_conf": "/etc/eni/eni_conf",
      "eni_max_pool_size": 25,
      "eni_min_pool_size": 10,
      "eni_vswitches": {
        "cn-hangzhou-h": ["vsw-xxx1"],
        "cn-hangzhou-i": ["vsw-xxx2"]
      }
    },
    {
      "type": "portmap",
      "capabilities": {"portMappings": true}
    }
  ]
}
```

## 1.2 Terway 核心架构

### 整体架构图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                            Terway 架构概览                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        Kubernetes Control Plane                      │   │
│  │  ┌────────────┐  ┌────────────┐  ┌──────────────────────────────┐   │   │
│  │  │ API Server │  │ Scheduler  │  │ Controller Manager           │   │   │
│  │  └────────────┘  └────────────┘  └──────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                      │                                      │
│                                      ▼                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         Worker Nodes                                │   │
│  │                                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                    kubelet + CRI                            │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │               │                                                     │   │
│  │               ▼                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                    CNI Plugin Chain                         │   │   │
│  │  │                                                             │   │   │
│  │  │  [terway] → [portmap] → [bandwidth] → [firewall]           │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │               │                                                     │   │
│  │               ▼                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                    Terway Components                        │   │   │
│  │  │  ┌─────────────────┐  ┌─────────────────┐  ┌────────────┐  │   │   │
│  │  │  │  terway-eniip   │  │ eniip-daemon    │  │ terway-cli │  │   │   │
│  │  │  │   (DaemonSet)   │  │   (DaemonSet)   │  │   (工具)   │  │   │   │
│  │  │  └─────────────────┘  └─────────────────┘  └────────────┘  │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │               │                                                     │   │
│  │               ▼                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                    Alibaba Cloud APIs                       │   │   │
│  │  │  ┌─────────────────┐  ┌─────────────────┐  ┌────────────┐  │   │   │
│  │  │  │  ECS API        │  │  VPC API        │  │  RAM API   │  │   │   │
│  │  │  └─────────────────┘  └─────────────────┘  └────────────┘  │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                      │                                      │
│              ┌───────────────────────┼───────────────────────┐              │
│              │                       │                       │              │
│              ▼                       ▼                       ▼              │
│  ┌─────────────────┐    ┌─────────────────┐    ┌───────────────────────┐   │
│  │    Pod Network  │    │   VPC Network   │    │    Security Groups    │   │
│  │                 │    │                 │    │                       │   │
│  │ • Pod IPs       │    │ • VSwitches     │    │ • Access Control      │   │
│  │ • Routes        │    │ • Route Tables  │    │ • Isolation           │   │
│  │ • Interfaces    │    │ • Gateways      │    │ • Rules               │   │
│  └─────────────────┘    └─────────────────┘    └───────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 核心组件详解

| 组件 | 功能描述 | 技术特点 |
|------|----------|----------|
| **terway-eniip** | 核心 CNI 插件 | 负责 Pod 网络接口创建和配置 |
| **eniip-daemon** | IP 管理守护进程 | 管理 ENI 和 IP 地址池，预热机制 |
| **terway-cli** | 命令行工具 | 网络状态查看和故障诊断 |
| **ConfigMap** | 配置管理 | 集中式网络配置管理 |
| **Cloud APIs** | 云服务接口 | 调用阿里云 ECS/VPC/RAM API |

## 1.3 Terway 网络模式对比

### 四种网络模式详解

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Terway 网络模式对比                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  模式名称        │ VPC路由    │ ENI独占    │ ENIIP共享   │ Trunking高密度  │
│  Pod IP来源     │ VPC路由表   │ 独占ENI    │ ENI辅助IP   │ ENI Trunk      │
│  性能表现       │ 高         │ 最高       │ 高         │ 高            │
│  容量限制       │ 路由条目    │ ENI配额    │ IP配额     │ VLAN配额      │
│  适用场景       │ 小规模     │ 高性能     │ 推荐通用   │ 超大规模      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 详细对比矩阵

| 对比维度 | VPC 模式 | ENI 模式 | ENIIP 模式 | Trunking 模式 |
|----------|----------|----------|------------|---------------|
| **Pod IP 来源** | VPC 路由分配 | 独占 ENI 主 IP | ENI 辅助 IP | Trunk ENI VLAN |
| **网络性能** | 高 (路由转发) | 最高 (直通) | 高 (直通) | 高 (直通) |
| **容量限制因素** | VPC 路由条目 | ECS ENI 配额 | ENI 辅助 IP 配额 | VLAN ID 配额 |
| **典型容量** | 50-100 Pods/节点 | 1-3 Pods/节点 | 20-100 Pods/节点 | 200+ Pods/节点 |
| **IP 消耗** | 低 (共享节点IP段) | 高 (1 Pod=1 ENI) | 中 (1 ENI=N IPs) | 最低 (1 Trunk=N VLANs) |
| **安全组支持** | 节点级别 | Pod 级别 | Pod 级别 | Pod 级别 |
| **固定 IP 支持** | ❌ | ✅ | ✅ | ✅ |
| **推荐使用场景** | 测试/小规模 | 高性能/隔离 | **生产推荐** | 超大规模集群 |

### 各模式工作原理

**VPC 路由模式**:
```
Pod → veth pair → linux bridge → VPC 路由表 → 目标 Pod
优点: 配置简单，IP 消耗少
缺点: 受路由条目限制，不适合大规模集群
```

**ENI 独占模式**:
```
Pod → veth pair → 独占 ENI → VPC 网络 → 目标 Pod
优点: 性能最高，支持 Pod 级别安全组
缺点: IP 消耗高，容量受限
```

**ENIIP 共享模式**:
```
Pod → veth pair → ENI 辅助 IP → VPC 网络 → 目标 Pod
优点: 性能高，容量大，支持安全组
缺点: 需要预热 IP 池
```

**Trunking 模式**:
```
Pod → veth pair → Trunk ENI + VLAN Tag → VPC 网络 → 目标 Pod
优点: 超高密度，IP 消耗最低
缺点: 配置复杂，需要 VLAN 支持
```

## 1.4 与主流 CNI 方案对比

### 技术架构对比

| 对比维度 | Terway | Calico | Flannel | Cilium |
|----------|--------|--------|---------|--------|
| **网络模型** | VPC/ENI | BGP/IPIP/VXLAN | VXLAN/host-gw | eBPF/VXLAN |
| **云原生支持** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐ |
| **性能表现** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **NetworkPolicy** | ✅ (Cilium后端) | ✅ (原生) | ❌ | ✅ (L3-L7) |
| **安全组集成** | ✅ (阿里云) | ❌ | ❌ | ❌ |
| **固定IP支持** | ✅ | ❌ | ❌ | ✅ |
| **多云支持** | 阿里云专属 | ✅ | ✅ | ✅ |
| **运维复杂度** | 中 | 高 | 低 | 高 |

### 选型决策矩阵

```
选择 CNI 方案?
├─ 云环境?
│  ├─ 阿里云 ACK?
│  │  ├─ 是 → Terway (强烈推荐)
│  │  └─ 否 → 继续判断
│  │
│  ├─ AWS/EKS?
│  │  ├─ 是 → AWS VPC CNI
│  │  └─ 否 → 继续判断
│  │
│  └─ 多云混合?
│     ├─ 是 → Calico/Cilium
│     └─ 否 → 继续判断
│
├─ 性能要求?
│  ├─ 极致性能 → Cilium eBPF
│  ├─ 高性能 → Terway ENI
│  └─ 一般性能 → Flannel
│
├─ 安全要求?
│  ├─ 高安全 → Calico + NetworkPolicy
│  ├─ 云安全组 → Terway
│  └─ 基础安全 → Flannel
│
├─ 运维能力?
│  ├─ 运维能力强 → Calico/Cilium
│  ├─ 运维能力中等 → Terway
│  └─ 运维能力有限 → Flannel
│
└─ 默认选择 → Terway (ACK环境)
```

## 1.5 Terway 发展历程

### 版本演进路线

```
2017 ── Terway 项目启动 (阿里云开源)
  ↓
2018 ── v1.0.0 发布，基础 ENI 支持
  ↓
2019 ── v1.2.0 ENIIP 模式成熟
  ↓
2020 ── v1.5.0 支持 NetworkPolicy
  ↓
2021 ── v1.7.0 集成 Cilium eBPF
  ↓
2022 ── v1.9.0 Trunking 模式支持
  ↓
2023 ── v1.10.0 性能大幅优化
  ↓
2024 ── v1.12.0 大规模集群支持
  ↓
至今 ── 持续演进，功能完善
```

### 关键里程碑

| 时间 | 里程碑 | 影响 |
|------|--------|------|
| 2019 | ACK 默认 CNI | 阿里云生态标准化 |
| 2021 | Cilium 集成 | 网络策略能力飞跃 |
| 2022 | Trunking 支持 | 超大规模部署成为可能 |
| 2023 | 性能优化 | 生产环境广泛采用 |
| 2024 | 多集群支持 | 企业级特性完善 |

---

*第一章完 - 掌握了 Terway 基础架构和核心概念*

---

# 第二章 Terway 网络模式详解

## 2.1 VPC 路由模式

### 工作原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          VPC 路由模式架构                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Node-1 (192.168.1.10)              Node-2 (192.168.1.20)                   │
│  ┌─────────────────────┐            ┌─────────────────────┐                 │
│  │  Pod-A              │            │  Pod-C              │                 │
│  │  10.0.1.2           │            │  10.0.2.2           │                 │
│  │  ┌───────────┐      │            │  ┌───────────┐      │                 │
│  │  │   eth0    │      │            │  │   eth0    │      │                 │
│  │  └─────┬─────┘      │            │  └─────┬─────┘      │                 │
│  │        │veth        │            │        │veth        │                 │
│  │  ┌─────┴─────┐      │            │  ┌─────┴─────┐      │                 │
│  │  │  cni0     │      │            │  │  cni0     │      │                 │
│  │  │  bridge   │      │            │  │  bridge   │      │                 │
│  │  └─────┬─────┘      │            │  └─────┬─────┘      │                 │
│  └────────┼────────────┘            └────────┼────────────┘                 │
│           │                                  │                               │
│           ▼                                  ▼                               │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         VPC 路由表                                   │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │  目标网段        │  下一跳         │  类型                   │   │   │
│  │  ├─────────────────────────────────────────────────────────────┤   │   │
│  │  │  10.0.1.0/24    │  192.168.1.10   │  ECS 实例               │   │   │
│  │  │  10.0.2.0/24    │  192.168.1.20   │  ECS 实例               │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 配置示例

```yaml
# VPC 路由模式 ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "max_pool_size": 5,
      "min_pool_size": 0,
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx"]
      },
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "network_mode": "VPC"
    }
```

### 优缺点分析

| 维度 | 说明 |
|------|------|
| **优点** | 配置简单、IP消耗少、易于理解和调试 |
| **缺点** | 受VPC路由条目限制(通常50条)、不支持Pod级安全组 |
| **适用场景** | 测试环境、小规模集群(<50节点) |

## 2.2 ENI 独占模式

### 工作原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          ENI 独占模式架构                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Node (ECS Instance)                                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │   │
│  │  │   Pod-A      │    │   Pod-B      │    │   Pod-C      │          │   │
│  │  │              │    │              │    │              │          │   │
│  │  └──────┬───────┘    └──────┬───────┘    └──────┬───────┘          │   │
│  │         │                   │                   │                   │   │
│  │         ▼                   ▼                   ▼                   │   │
│  │  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │   │
│  │  │   ENI-1      │    │   ENI-2      │    │   ENI-3      │          │   │
│  │  │ 独占 (eth1)  │    │ 独占 (eth2)  │    │ 独占 (eth3)  │          │   │
│  │  │ 10.0.1.100   │    │ 10.0.1.101   │    │ 10.0.1.102   │          │   │
│  │  │ SG: sg-pod-a │    │ SG: sg-pod-b │    │ SG: sg-pod-c │          │   │
│  │  └──────┬───────┘    └──────┬───────┘    └──────┬───────┘          │   │
│  │         │                   │                   │                   │   │
│  └─────────┼───────────────────┼───────────────────┼───────────────────┘   │
│            │                   │                   │                       │
│            └───────────────────┼───────────────────┘                       │
│                                ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         VPC 网络                                    │   │
│  │                    (直接 VPC 级别通信)                               │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 配置示例

```yaml
# ENI 独占模式配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "max_pool_size": 3,
      "min_pool_size": 1,
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx"]
      },
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "network_mode": "ENI"
    }
---
# Pod 指定安全组
apiVersion: v1
kind: Pod
metadata:
  name: high-security-pod
  annotations:
    k8s.aliyun.com/eni: "true"
    k8s.aliyun.com/security-group: "sg-bp1high-security"
spec:
  containers:
  - name: app
    image: nginx:1.24
```

### ENI 配额限制

| ECS 规格 | 最大 ENI 数 | 每 ENI 辅助 IP | 可用 Pod 数(ENI模式) |
|----------|-------------|----------------|----------------------|
| ecs.g6.large | 3 | 10 | 2 (除主ENI) |
| ecs.g6.xlarge | 4 | 15 | 3 |
| ecs.g6.2xlarge | 4 | 15 | 3 |
| ecs.g6.4xlarge | 8 | 30 | 7 |
| ecs.g6.8xlarge | 8 | 30 | 7 |

## 2.3 ENIIP 共享模式 (推荐)

### 工作原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        ENIIP 共享模式架构 (推荐)                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Node (ECS Instance)                                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐            │   │
│  │  │ Pod-1  │ │ Pod-2  │ │ Pod-3  │ │ Pod-4  │ │ Pod-5  │            │   │
│  │  │10.0.1.2│ │10.0.1.3│ │10.0.1.4│ │10.0.1.5│ │10.0.1.6│            │   │
│  │  └───┬────┘ └───┬────┘ └───┬────┘ └───┬────┘ └───┬────┘            │   │
│  │      │          │          │          │          │                  │   │
│  │      └──────────┴────┬─────┴──────────┴──────────┘                  │   │
│  │                      ▼                                              │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                      ENI-1 (共享模式)                        │   │   │
│  │  │  主IP: 10.0.1.1                                              │   │   │
│  │  │  辅助IPs: 10.0.1.2, 10.0.1.3, 10.0.1.4, 10.0.1.5, 10.0.1.6   │   │   │
│  │  │  安全组: sg-bp1xxx                                           │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                    IP 地址池 (eniip-daemon 管理)              │   │   │
│  │  │  预热池大小: min_pool_size=10                                 │   │   │
│  │  │  最大池大小: max_pool_size=25                                 │   │   │
│  │  │  当前可用: 5 IPs | 已分配: 5 IPs                              │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 配置示例

```yaml
# ENIIP 模式 ConfigMap (推荐生产配置)
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "max_pool_size": 25,
      "min_pool_size": 10,
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx", "vsw-bp2xxx"],
        "cn-hangzhou-i": ["vsw-bp3xxx"]
      },
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "network_mode": "ENIIP",
      "eni_tags": {
        "Environment": "production",
        "ManagedBy": "terway"
      }
    }
  10-terway.conf: |
    {
      "cniVersion": "0.4.0",
      "name": "terway",
      "type": "terway",
      "eniip_virtual_type": "IPVlan",
      "ip_stack": "ipv4"
    }
```

### IP 池管理机制

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          IP 池预热与回收机制                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  时间轴 ─────────────────────────────────────────────────────────────────→  │
│                                                                             │
│  ┌─────────┐  Pod创建  ┌─────────┐  业务高峰  ┌─────────┐  业务低谷        │
│  │ 启动预热 │ ────────→ │ IP分配  │ ────────→ │ 动态扩展 │ ────────→       │
│  │ 10 IPs  │          │ 使用池IP │          │ 25 IPs  │                   │
│  └─────────┘          └─────────┘          └─────────┘                   │
│       │                    │                    │                          │
│       ▼                    ▼                    ▼                          │
│  min_pool_size=10    快速Pod启动           max_pool_size=25               │
│  (启动时预热)         (无需等待API)         (按需扩展上限)                   │
│                                                                             │
│  回收策略:                                                                   │
│  ├─ Pod删除后IP回收到池                                                     │
│  ├─ 超过max_pool_size的IP释放回VPC                                         │
│  └─ 空闲超时(默认5min)后缩减至min_pool_size                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### ENIIP 容量规划

| ECS 规格 | 最大 ENI 数 | 每ENI辅助IP | 最大Pod数(ENIIP) | 推荐配置 |
|----------|-------------|-------------|------------------|----------|
| ecs.g7.large | 3 | 10 | 29 | min=5,max=15 |
| ecs.g7.xlarge | 4 | 15 | 59 | min=10,max=30 |
| ecs.g7.2xlarge | 4 | 15 | 59 | min=15,max=40 |
| ecs.g7.4xlarge | 8 | 30 | 239 | min=20,max=60 |
| ecs.g7.8xlarge | 8 | 30 | 239 | min=30,max=80 |

## 2.4 Trunking 高密度模式

### 工作原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Trunking 高密度模式架构                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Node (ECS Instance - 需要支持 Trunk ENI 的实例规格)                          │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  ┌────────┐ ┌────────┐ ┌────────┐        ┌────────┐ ┌────────┐     │   │
│  │  │ Pod-1  │ │ Pod-2  │ │ Pod-3  │  ...   │Pod-199 │ │Pod-200 │     │   │
│  │  │VLAN 101│ │VLAN 102│ │VLAN 103│        │VLAN 299│ │VLAN 300│     │   │
│  │  └───┬────┘ └───┬────┘ └───┬────┘        └───┬────┘ └───┬────┘     │   │
│  │      │          │          │                  │          │          │   │
│  │      └──────────┴──────────┴───────┬──────────┴──────────┘          │   │
│  │                                    │                                 │   │
│  │  ┌─────────────────────────────────┴─────────────────────────────┐  │   │
│  │  │                    Trunk ENI (主干网卡)                        │  │   │
│  │  │  ┌─────────────────────────────────────────────────────────┐  │  │   │
│  │  │  │  VLAN 101 ──→ Member ENI 1 ──→ 10.0.1.1 (Pod-1)        │  │  │   │
│  │  │  │  VLAN 102 ──→ Member ENI 2 ──→ 10.0.1.2 (Pod-2)        │  │  │   │
│  │  │  │  VLAN 103 ──→ Member ENI 3 ──→ 10.0.1.3 (Pod-3)        │  │  │   │
│  │  │  │  ...                                                    │  │  │   │
│  │  │  │  VLAN 300 ──→ Member ENI 200 ──→ 10.0.1.200 (Pod-200)  │  │  │   │
│  │  │  └─────────────────────────────────────────────────────────┘  │  │   │
│  │  └───────────────────────────────────────────────────────────────┘  │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  Trunking 优势:                                                             │
│  ├─ 突破 ENI 数量限制 (通过 VLAN 扩展)                                       │
│  ├─ 每个 Pod 独立安全组                                                     │
│  ├─ 支持 200+ Pods/节点                                                     │
│  └─ 适用于 Serverless 等高密度场景                                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 配置示例

```yaml
# Trunking 模式配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "max_pool_size": 50,
      "min_pool_size": 20,
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx"]
      },
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "network_mode": "Trunk",
      "trunk_on": true,
      "eni_capability": {
        "trunk": true,
        "member_adapters": 200
      }
    }
```

### 支持 Trunking 的实例规格

| 实例系列 | 规格示例 | 最大 Member ENI | 适用场景 |
|----------|----------|-----------------|----------|
| ecs.ebmg7 | ecs.ebmg7.32xlarge | 256 | 裸金属高密度 |
| ecs.g7ne | ecs.g7ne.24xlarge | 200 | 网络增强型 |
| ecs.c7 | ecs.c7.16xlarge | 128 | 计算密集型 |
| ecs.r7 | ecs.r7.16xlarge | 128 | 内存密集型 |

## 2.5 网络模式选型指南

### 决策流程图

```
网络模式选型?
│
├─ 集群规模?
│  ├─ 小规模 (<50节点)
│  │  └─ 测试/开发 → VPC 路由模式
│  │
│  ├─ 中等规模 (50-200节点)
│  │  └─ 生产环境 → ENIIP 模式 (推荐)
│  │
│  └─ 大规模 (>200节点)
│     └─ 超高密度 → Trunking 模式
│
├─ 安全要求?
│  ├─ Pod级别安全组
│  │  ├─ 低成本 → ENIIP 模式
│  │  └─ 极致隔离 → ENI 独占模式
│  │
│  └─ 节点级别安全组
│     └─ VPC 路由模式
│
├─ 性能要求?
│  ├─ 极致网络性能
│  │  └─ ENI 独占模式 (直接VPC通信)
│  │
│  └─ 标准性能
│     └─ ENIIP 模式
│
└─ 默认推荐 → ENIIP 模式
```

### 模式切换注意事项

```yaml
# 重要提醒: 网络模式切换需要重建集群或节点池
# 以下是切换前的检查清单

# 1. 备份当前配置
kubectl get configmap eni-config -n kube-system -o yaml > eni-config-backup.yaml

# 2. 检查 Pod IP 分配情况
kubectl get pods -A -o wide | grep -v Completed

# 3. 评估 IP 地址消耗
kubectl exec -n kube-system <terway-pod> -c terway -- terway-cli show

# 4. 确认节点规格支持目标模式
# ENI 独占: 检查 ECS ENI 配额
# Trunking: 确认实例规格支持 Trunk ENI
```

---

*第二章完 - 掌握了 Terway 四种网络模式的原理和配置*

---

# 第三章 Terway 在 Kubernetes 中的集成

## 3.1 ACK 集群网络架构

### 整体网络拓扑

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        ACK 集群网络架构全景图                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        VPC (10.0.0.0/8)                             │   │
│  │                                                                     │   │
│  │  ┌───────────────────┐  ┌───────────────────┐  ┌─────────────────┐ │   │
│  │  │   可用区 A        │  │   可用区 B        │  │   可用区 C      │ │   │
│  │  │   VSwitch-A       │  │   VSwitch-B       │  │   VSwitch-C     │ │   │
│  │  │   10.0.1.0/24     │  │   10.0.2.0/24     │  │   10.0.3.0/24   │ │   │
│  │  │                   │  │                   │  │                 │ │   │
│  │  │  ┌─────────────┐  │  │  ┌─────────────┐  │  │  ┌───────────┐  │ │   │
│  │  │  │  Master-1   │  │  │  │  Master-2   │  │  │  │  Master-3 │  │ │   │
│  │  │  │  (托管)     │  │  │  │  (托管)     │  │  │  │  (托管)   │  │ │   │
│  │  │  └─────────────┘  │  │  └─────────────┘  │  │  └───────────┘  │ │   │
│  │  │                   │  │                   │  │                 │ │   │
│  │  │  ┌─────────────┐  │  │  ┌─────────────┐  │  │  ┌───────────┐  │ │   │
│  │  │  │  Worker-1   │  │  │  │  Worker-2   │  │  │  │  Worker-3 │  │ │   │
│  │  │  │  Node Pool  │  │  │  │  Node Pool  │  │  │  │  Node Pool│  │ │   │
│  │  │  │             │  │  │  │             │  │  │  │           │  │ │   │
│  │  │  │ Pod CIDR:   │  │  │  │ Pod CIDR:   │  │  │  │ Pod CIDR: │  │ │   │
│  │  │  │ 10.0.1.0/25 │  │  │  │ 10.0.2.0/25 │  │  │  │ 10.0.3.0/25│ │ │   │
│  │  │  └─────────────┘  │  │  └─────────────┘  │  │  └───────────┘  │ │   │
│  │  │                   │  │                   │  │                 │ │   │
│  │  └───────────────────┘  └───────────────────┘  └─────────────────┘ │   │
│  │                                                                     │   │
│  │  网络组件:                                                           │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌────────────┐ │   │
│  │  │   Terway    │  │  CoreDNS    │  │  kube-proxy │  │ Ingress    │ │   │
│  │  │   CNI       │  │  集群DNS    │  │  Service代理 │  │ Controller │ │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └────────────┘ │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  Service CIDR: 172.16.0.0/16 (独立于 VPC)                                   │
│  Pod CIDR: 使用 VPC 子网 IP (Terway 特性)                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 网络平面划分

| 网络平面 | CIDR 示例 | 用途 | 管理组件 |
|----------|-----------|------|----------|
| **VPC 网络** | 10.0.0.0/8 | 底层 VPC 网络 | 阿里云 VPC |
| **节点网络** | 10.0.x.0/24 | 各可用区节点通信 | VSwitch |
| **Pod 网络** | VPC IP | Pod 间通信 | Terway |
| **Service 网络** | 172.16.0.0/16 | 服务发现和负载均衡 | kube-proxy |

## 3.2 Pod IP 分配机制

### IP 分配流程

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Pod IP 分配流程                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. Pod 创建请求                                                            │
│  ┌─────────────┐                                                            │
│  │  kubectl    │──create pod──→ API Server ──→ Scheduler ──→ Node          │
│  └─────────────┘                                                            │
│                                                                             │
│  2. kubelet 调用 CNI                                                        │
│  ┌─────────────┐         ┌─────────────┐         ┌─────────────┐            │
│  │   kubelet   │──ADD──→│  CNI Chain  │──────→│   Terway    │            │
│  └─────────────┘         └─────────────┘         └──────┬──────┘            │
│                                                         │                    │
│  3. Terway 分配 IP                                      ▼                    │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                       eniip-daemon                                   │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │  IP 池检查                                                    │   │   │
│  │  │  ├─ 池中有可用 IP? ──Yes──→ 直接分配 (毫秒级)                  │   │   │
│  │  │  └─ 池中无可用 IP? ──No───→ 调用阿里云 API 申请新 IP (秒级)    │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  4. 网络配置                                                                │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Terway 配置网络接口                                                 │   │
│  │  ├─ 创建 veth pair                                                   │   │
│  │  ├─ 配置 IP 地址                                                     │   │
│  │  ├─ 设置路由规则                                                     │   │
│  │  ├─ 配置策略路由 (ENIIP 模式)                                        │   │
│  │  └─ 返回成功                                                         │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  5. Pod 就绪                                                                │
│  ┌─────────────┐                                                            │
│  │    Pod      │ Ready, IP: 10.0.1.100                                     │
│  └─────────────┘                                                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### IP 分配策略

```yaml
# 查看节点 IP 分配状态
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.k8s\.aliyun\.com/pod-bindable}{"\n"}{end}'

# 查看具体节点的 ENI 和 IP 信息
kubectl exec -n kube-system <terway-pod> -c terway -- terway-cli show

# 输出示例:
# Node: cn-hangzhou.10.0.1.10
# ENI: eni-bp1xxx (eth1)
#   Primary IP: 10.0.1.10
#   Secondary IPs:
#     - 10.0.1.100 (allocated to pod-a)
#     - 10.0.1.101 (allocated to pod-b)
#     - 10.0.1.102 (available)
#     - 10.0.1.103 (available)
# IP Pool: 4 total, 2 allocated, 2 available
```

### 固定 IP 配置 (StatefulSet)

```yaml
# StatefulSet 固定 IP 配置
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  annotations:
    # 启用固定 IP
    k8s.aliyun.com/pod-bindable: "true"
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
      annotations:
        # Pod 级别固定 IP 注解
        k8s.aliyun.com/pod-bindable: "true"
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
```

## 3.3 与 kube-proxy 协同工作

### 协同架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     Terway 与 kube-proxy 协同架构                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        数据平面                                      │   │
│  │                                                                     │   │
│  │  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐         │   │
│  │  │   Pod-A     │      │   Service   │      │   Pod-B     │         │   │
│  │  │ 10.0.1.100  │─────→│  ClusterIP  │─────→│ 10.0.2.100  │         │   │
│  │  └─────────────┘      │ 172.16.1.1  │      └─────────────┘         │   │
│  │                       └──────┬──────┘                               │   │
│  │                              │                                      │   │
│  │         ┌────────────────────┼────────────────────┐                 │   │
│  │         │                    │                    │                 │   │
│  │         ▼                    ▼                    ▼                 │   │
│  │  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐         │   │
│  │  │ kube-proxy  │      │   Terway    │      │   eBPF      │         │   │
│  │  │  (IPVS)     │      │   (CNI)     │      │  (Cilium)   │         │   │
│  │  └─────────────┘      └─────────────┘      └─────────────┘         │   │
│  │         │                    │                    │                 │   │
│  │         ▼                    ▼                    ▼                 │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │                    Linux Kernel                              │   │   │
│  │  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐         │   │   │
│  │  │  │ iptables│  │  IPVS   │  │ netfilter│  │   XDP   │         │   │   │
│  │  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘         │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  职责划分:                                                                   │
│  ├─ Terway: Pod 网络接口配置、IP 分配、VPC 集成                              │
│  ├─ kube-proxy: Service 到 Pod 的流量转发 (IPVS/iptables)                   │
│  └─ Cilium (可选): eBPF 加速、L7 策略、替代 kube-proxy                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### kube-proxy 模式配置

```yaml
# kube-proxy ConfigMap (IPVS 模式 - 推荐)
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    mode: "ipvs"
    ipvs:
      scheduler: "rr"          # 调度算法: rr, lc, dh, sh, sed, nq
      syncPeriod: "30s"
      minSyncPeriod: "2s"
      tcpTimeout: "0s"
      tcpFinTimeout: "0s"
      udpTimeout: "0s"
    conntrack:
      maxPerCore: 32768
      min: 131072
```

### eBPF 加速模式 (Cilium)

```yaml
# 启用 Terway + Cilium eBPF 加速
# 集群创建时选择或通过组件升级启用

# 验证 eBPF 模式
kubectl get pods -n kube-system -l k8s-app=cilium

# 检查 Cilium 状态
kubectl exec -n kube-system <cilium-pod> -- cilium status

# eBPF 性能优势:
# - Service 负载均衡绕过 iptables/IPVS
# - L7 策略在内核态执行
# - 网络延迟降低 30-50%
# - CPU 消耗降低 40-60%
```

## 3.4 多可用区网络规划

### 跨可用区通信

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          多可用区网络规划                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  可用区 A (cn-hangzhou-h)        可用区 B (cn-hangzhou-i)                   │
│  ┌─────────────────────┐         ┌─────────────────────┐                    │
│  │  VSwitch-A          │         │  VSwitch-B          │                    │
│  │  10.0.1.0/24        │         │  10.0.2.0/24        │                    │
│  │                     │         │                     │                    │
│  │  ┌───────────────┐  │         │  ┌───────────────┐  │                    │
│  │  │  Node Pool A  │  │         │  │  Node Pool B  │  │                    │
│  │  │               │  │◀───────▶│  │               │  │                    │
│  │  │  Pod CIDR:    │  │   VPC   │  │  Pod CIDR:    │  │                    │
│  │  │  10.0.1.0/25  │  │  路由   │  │  10.0.2.0/25  │  │                    │
│  │  └───────────────┘  │         │  └───────────────┘  │                    │
│  │                     │         │                     │                    │
│  └─────────────────────┘         └─────────────────────┘                    │
│                                                                             │
│  规划要点:                                                                   │
│  ├─ 每个可用区独立 VSwitch                                                  │
│  ├─ Pod CIDR 按可用区划分                                                   │
│  ├─ 避免跨可用区流量收费 (使用拓扑感知调度)                                   │
│  └─ 关键服务部署多可用区副本                                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 拓扑感知配置

```yaml
# 拓扑感知 Service (优先本可用区流量)
apiVersion: v1
kind: Service
metadata:
  name: app-service
  annotations:
    service.kubernetes.io/topology-aware-hints: "Auto"
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
  topologyKeys:
  - "topology.kubernetes.io/zone"
  - "kubernetes.io/hostname"
  - "*"
---
# Pod 拓扑分布约束
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 6
  template:
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: myapp
      containers:
      - name: app
        image: myapp:v1
```

---

*第三章完 - 掌握了 Terway 与 Kubernetes 的集成机制*

---

# 第四章 ACK 环境 Terway 配置管理

## 4.1 集群创建时网络配置

### 创建集群网络选项

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      ACK 集群创建 - 网络配置选项                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  步骤 1: 选择网络插件                                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  ○ Flannel (简单场景)                                                │   │
│  │  ● Terway (推荐 - 阿里云原生)                                        │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  步骤 2: 选择 Terway 模式                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  ○ VPC 路由模式 (受路由条目限制)                                      │   │
│  │  ● ENIIP 模式 (推荐 - 高性能大容量)                                   │   │
│  │  ○ ENI 独占模式 (高隔离需求)                                         │   │
│  │  ○ Trunking 模式 (超高密度)                                          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  步骤 3: 配置 Pod 网段                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Pod 虚拟交换机:                                                      │   │
│  │    可用区 A: vsw-bp1xxx (10.0.1.0/24) ✓                              │   │
│  │    可用区 B: vsw-bp2xxx (10.0.2.0/24) ✓                              │   │
│  │    可用区 C: vsw-bp3xxx (10.0.3.0/24) ✓                              │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  步骤 4: 配置 Service CIDR                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Service CIDR: 172.16.0.0/16                                         │   │
│  │  (不能与 VPC CIDR 重叠)                                               │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 使用 CLI 创建集群

```bash
# 使用阿里云 CLI 创建 ACK 集群 (Terway ENIIP 模式)
aliyun cs POST /clusters --body '{
  "name": "production-cluster",
  "region_id": "cn-hangzhou",
  "cluster_type": "ManagedKubernetes",
  "kubernetes_version": "1.28.3-aliyun.1",
  "vpcid": "vpc-bp1xxx",
  "container_cidr": "",
  "service_cidr": "172.16.0.0/16",
  "addons": [
    {
      "name": "terway-eniip",
      "config": "{\"IPVlan\":\"true\",\"NetworkPolicy\":\"true\"}"
    },
    {
      "name": "csi-plugin"
    },
    {
      "name": "csi-provisioner"
    }
  ],
  "worker_vswitch_ids": ["vsw-bp1xxx", "vsw-bp2xxx"],
  "pod_vswitch_ids": ["vsw-bp3xxx", "vsw-bp4xxx"],
  "num_of_nodes": 3,
  "worker_instance_types": ["ecs.g7.2xlarge"],
  "worker_system_disk_category": "cloud_essd",
  "worker_system_disk_size": 120
}'
```

## 4.2 ConfigMap 详细配置参数

### 核心配置文件

```yaml
# Terway 核心配置 ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "access_key": "",
      "access_secret": "",
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx", "vsw-bp2xxx"],
        "cn-hangzhou-i": ["vsw-bp3xxx"]
      },
      "max_pool_size": 25,
      "min_pool_size": 10,
      "eni_tags": {
        "Environment": "production",
        "ManagedBy": "terway",
        "Cluster": "prod-ack-01"
      }
    }
  10-terway.conf: |
    {
      "cniVersion": "0.4.0",
      "name": "terway",
      "type": "terway",
      "eniip_virtual_type": "IPVlan",
      "ip_stack": "ipv4"
    }
  disable_network_policy: "false"
```

### 配置参数详解

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `version` | string | "1" | 配置版本 |
| `security_group` | string | 必填 | 默认安全组 ID |
| `service_cidr` | string | 必填 | Service 网段 |
| `vswitches` | object | 必填 | 各可用区 VSwitch 列表 |
| `max_pool_size` | int | 25 | IP 池最大容量 |
| `min_pool_size` | int | 10 | IP 池最小容量(预热) |
| `eni_tags` | object | {} | ENI 资源标签 |
| `eniip_virtual_type` | string | "IPVlan" | 虚拟化类型: IPVlan/Veth |
| `ip_stack` | string | "ipv4" | IP 协议栈: ipv4/dual |

### 高级配置选项

```yaml
# 高级 Terway 配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx"]
      },
      "max_pool_size": 50,
      "min_pool_size": 20,
      
      // 高级选项
      "eni_cap_policy": "prefer_trunk",
      "enable_eni_trunking": true,
      "trunk_eni_count": 2,
      
      // IP 分配策略
      "ip_allocation_policy": {
        "prefer_eni_with_most_ips": true,
        "ip_reserve_percent": 20
      },
      
      // 资源回收
      "resource_gc": {
        "enable": true,
        "interval_seconds": 300,
        "orphan_eni_ttl_seconds": 600
      },
      
      // API 限流
      "rate_limit": {
        "eni_create_qps": 5,
        "eni_delete_qps": 5,
        "ip_assign_qps": 20
      }
    }
```

## 4.3 多可用区网络规划

### VSwitch 规划策略

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        多可用区 VSwitch 规划                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  VPC: 10.0.0.0/8                                                            │
│                                                                             │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │   可用区 A      │  │   可用区 B      │  │   可用区 C      │              │
│  │   cn-hangzhou-h │  │   cn-hangzhou-i │  │   cn-hangzhou-j │              │
│  │                 │  │                 │  │                 │              │
│  │  节点 VSwitch:  │  │  节点 VSwitch:  │  │  节点 VSwitch:  │              │
│  │  10.0.1.0/24    │  │  10.0.2.0/24    │  │  10.0.3.0/24    │              │
│  │  (254 节点)     │  │  (254 节点)     │  │  (254 节点)     │              │
│  │                 │  │                 │  │                 │              │
│  │  Pod VSwitch-1: │  │  Pod VSwitch-1: │  │  Pod VSwitch-1: │              │
│  │  10.0.11.0/20   │  │  10.0.21.0/20   │  │  10.0.31.0/20   │              │
│  │  (4094 IPs)     │  │  (4094 IPs)     │  │  (4094 IPs)     │              │
│  │                 │  │                 │  │                 │              │
│  │  Pod VSwitch-2: │  │  Pod VSwitch-2: │  │  Pod VSwitch-2: │              │
│  │  10.0.12.0/20   │  │  10.0.22.0/20   │  │  10.0.32.0/20   │              │
│  │  (4094 IPs)     │  │  (4094 IPs)     │  │  (4094 IPs)     │              │
│  │                 │  │                 │  │                 │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
│                                                                             │
│  规划原则:                                                                   │
│  ├─ 每可用区至少 2 个 Pod VSwitch (备份)                                    │
│  ├─ Pod VSwitch 网段要足够大 (/20 或更大)                                   │
│  ├─ 预留 20-30% IP 作为扩展空间                                             │
│  └─ 节点和 Pod VSwitch 分离管理                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### IP 容量规划

```bash
# IP 容量计算公式
# 总 Pod 容量 = 节点数 × 每节点 Pod 数
# 每节点 Pod 数 = (ENI数 - 1) × 每ENI辅助IP数 + 主ENI辅助IP数

# 示例: ecs.g7.4xlarge 规格
# ENI 数: 8
# 每 ENI 辅助 IP: 30
# 每节点 Pod 数: (8-1) × 30 + 30 = 240 Pods

# 100 节点集群容量计算
# 最大 Pod 数: 100 × 240 = 24000 Pods
# 推荐 Pod VSwitch 网段: /14 (约 262000 IPs)
```

### 容量规划配置

```yaml
# 生产环境容量规划配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "security_group": "sg-bp1prod",
      "service_cidr": "172.16.0.0/16",
      "vswitches": {
        "cn-hangzhou-h": [
          "vsw-bp1xxx-pod-1",
          "vsw-bp1xxx-pod-2"
        ],
        "cn-hangzhou-i": [
          "vsw-bp2xxx-pod-1",
          "vsw-bp2xxx-pod-2"
        ],
        "cn-hangzhou-j": [
          "vsw-bp3xxx-pod-1",
          "vsw-bp3xxx-pod-2"
        ]
      },
      "max_pool_size": 60,
      "min_pool_size": 30,
      "eni_tags": {
        "Environment": "production",
        "CostCenter": "platform-infra"
      }
    }
```

## 4.4 节点池网络配置

### 节点池专属网络配置

```yaml
# 创建专属网络配置的节点池
# 通过阿里云控制台或 CLI

# CLI 示例
aliyun cs POST /clusters/{cluster_id}/nodepools --body '{
  "nodepool_info": {
    "name": "high-performance-pool"
  },
  "scaling_group": {
    "vswitch_ids": ["vsw-bp1xxx"],
    "instance_types": ["ecs.g7ne.4xlarge"],
    "system_disk_category": "cloud_essd",
    "system_disk_size": 120
  },
  "kubernetes_config": {
    "labels": [
      {"key": "node-pool", "value": "high-perf"}
    ],
    "taints": [
      {"key": "workload", "value": "high-perf", "effect": "NoSchedule"}
    ]
  },
  "nodepool_config": {
    "node_annotations": {
      "k8s.aliyun.com/eni-max-pool-size": "80",
      "k8s.aliyun.com/eni-min-pool-size": "40"
    }
  }
}'
```

### 节点级别 IP 池调整

```bash
# 调整单个节点的 IP 池大小 (通过节点注解)
kubectl annotate node cn-hangzhou.10.0.1.10 \
  k8s.aliyun.com/eni-max-pool-size=100 \
  k8s.aliyun.com/eni-min-pool-size=50 \
  --overwrite

# 验证配置生效
kubectl get node cn-hangzhou.10.0.1.10 -o jsonpath='{.metadata.annotations}' | jq .

# 查看节点实际 IP 池状态
kubectl exec -n kube-system <terway-pod> -c terway -- terway-cli show
```

---

*第四章完 - 掌握了 ACK 环境 Terway 配置管理*

---

# 第五章 高级特性与安全配置

## 5.1 固定 IP 配置

### StatefulSet 固定 IP

```yaml
# StatefulSet 固定 IP 完整配置
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
  namespace: database
spec:
  serviceName: redis-cluster
  replicas: 6
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: redis-cluster
  template:
    metadata:
      labels:
        app: redis-cluster
      annotations:
        # 核心: 启用固定 IP
        k8s.aliyun.com/pod-bindable: "true"
    spec:
      terminationGracePeriodSeconds: 30
      containers:
      - name: redis
        image: redis:7.2-alpine
        ports:
        - containerPort: 6379
          name: client
        - containerPort: 16379
          name: gossip
        command:
        - redis-server
        - /etc/redis/redis.conf
        - --cluster-enabled yes
        - --cluster-config-file /data/nodes.conf
        - --cluster-node-timeout 5000
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /etc/redis
      volumes:
      - name: config
        configMap:
          name: redis-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: alicloud-disk-essd
      resources:
        requests:
          storage: 50Gi
---
# Headless Service
apiVersion: v1
kind: Service
metadata:
  name: redis-cluster
  namespace: database
spec:
  clusterIP: None
  selector:
    app: redis-cluster
  ports:
  - port: 6379
    name: client
  - port: 16379
    name: gossip
```

### 固定 IP 工作原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          固定 IP 工作原理                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  首次创建 Pod:                                                              │
│  ┌─────────────┐                  ┌─────────────┐                          │
│  │  Pod-0      │ ──分配IP──────→ │  Terway     │ ──记录映射──→             │
│  │  redis-0    │    10.0.1.100   │  Controller │                           │
│  └─────────────┘                  └──────┬──────┘                          │
│                                          │                                  │
│                                          ▼                                  │
│                                   ┌─────────────┐                          │
│                                   │  CRD 存储   │                          │
│                                   │  PodNetInfo │                          │
│                                   │             │                          │
│                                   │ redis-0:    │                          │
│                                   │ 10.0.1.100  │                          │
│                                   └─────────────┘                          │
│                                                                             │
│  Pod 重建:                                                                  │
│  ┌─────────────┐                  ┌─────────────┐                          │
│  │  Pod-0      │ ──查询IP──────→ │  Terway     │ ──查询CRD──→              │
│  │  redis-0    │                 │  Controller │                           │
│  │  (重建)     │ ◀──恢复IP────── │             │ ◀──返回IP──               │
│  │  10.0.1.100 │    10.0.1.100   └─────────────┘                          │
│  └─────────────┘                                                            │
│                                                                             │
│  IP 保留条件:                                                               │
│  ├─ Pod 名称不变 (StatefulSet 特性)                                         │
│  ├─ PodNetInfo CRD 存在                                                    │
│  ├─ IP 未被其他 Pod 占用                                                    │
│  └─ 相同可用区 VSwitch 有效                                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 查看固定 IP 信息

```bash
# 查看 PodNetInfo CRD
kubectl get podnetinfos.network.alibabacloud.com -n database

# 查看具体 Pod 的网络信息
kubectl get podnetinfo redis-cluster-0 -n database -o yaml

# 输出示例:
# apiVersion: network.alibabacloud.com/v1
# kind: PodNetInfo
# metadata:
#   name: redis-cluster-0
#   namespace: database
# spec:
#   podIP: 10.0.1.100
#   eniID: eni-bp1xxx
#   macAddress: 00:16:3e:xx:xx:xx
#   vSwitchID: vsw-bp1xxx
#   securityGroupIDs:
#   - sg-bp1xxx
```

## 5.2 NetworkPolicy 策略管理

### NetworkPolicy 架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      Terway NetworkPolicy 架构                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Kubernetes API Server                           │   │
│  │                                                                     │   │
│  │  NetworkPolicy ──→ Terway Controller ──→ Cilium Agent ──→ eBPF     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  策略执行层:                                                                │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         eBPF 数据平面                                │   │
│  │                                                                     │   │
│  │  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐         │   │
│  │  │   Ingress   │      │   Egress    │      │    L7       │         │   │
│  │  │   Policy    │      │   Policy    │      │   Policy    │         │   │
│  │  │             │      │             │      │  (Cilium)   │         │   │
│  │  │ 入站流量控制 │      │ 出站流量控制 │      │ HTTP/gRPC  │         │   │
│  │  └─────────────┘      └─────────────┘      └─────────────┘         │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 常用 NetworkPolicy 示例

```yaml
# 1. 默认拒绝所有入站流量
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
---
# 2. 允许同 Namespace 内部通信
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
---
# 3. 允许特定标签的 Pod 访问数据库
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-app-to-db
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: mysql
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          access: database
    ports:
    - protocol: TCP
      port: 3306
---
# 4. 限制 Pod 出站访问
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Egress
  egress:
  # 允许 DNS 查询
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # 允许访问数据库
  - to:
    - podSelector:
        matchLabels:
          app: mysql
    ports:
    - protocol: TCP
      port: 3306
  # 允许访问外部 API
  - to:
    - ipBlock:
        cidr: 100.100.0.0/16
    ports:
    - protocol: TCP
      port: 443
---
# 5. 跨命名空间访问控制
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9090
```

### Cilium L7 策略 (高级)

```yaml
# Cilium L7 HTTP 策略
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: api-gateway-l7
  namespace: production
spec:
  endpointSelector:
    matchLabels:
      app: api-gateway
  ingress:
  - fromEndpoints:
    - matchLabels:
        app: frontend
    toPorts:
    - ports:
      - port: "8080"
        protocol: TCP
      rules:
        http:
        - method: "GET"
          path: "/api/v1/.*"
        - method: "POST"
          path: "/api/v1/orders"
          headers:
          - 'Content-Type: application/json'
```

## 5.3 安全组集成与访问控制

### Pod 级别安全组

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Pod 级别安全组配置                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  场景: 不同业务 Pod 使用不同安全组                                           │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                          VPC 网络                                    │   │
│  │                                                                     │   │
│  │  ┌───────────────┐    ┌───────────────┐    ┌───────────────┐       │   │
│  │  │   前端 Pod    │    │   后端 Pod    │    │   数据库 Pod   │       │   │
│  │  │               │    │               │    │               │       │   │
│  │  │ sg-frontend   │    │ sg-backend    │    │ sg-database   │       │   │
│  │  │ 入站: 80,443  │───▶│ 入站: 8080    │───▶│ 入站: 3306    │       │   │
│  │  │ 从 ALB        │    │ 从 sg-frontend│    │ 从 sg-backend │       │   │
│  │  └───────────────┘    └───────────────┘    └───────────────┘       │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 配置 Pod 安全组

```yaml
# 前端 Pod - 使用前端安全组
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
      annotations:
        # 指定安全组
        k8s.aliyun.com/security-groups: "sg-bp1frontend"
    spec:
      containers:
      - name: nginx
        image: nginx:1.24
        ports:
        - containerPort: 80
---
# 后端 Pod - 使用后端安全组
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
      annotations:
        k8s.aliyun.com/security-groups: "sg-bp1backend"
    spec:
      containers:
      - name: app
        image: myapp:v1
        ports:
        - containerPort: 8080
---
# 数据库 Pod - 使用数据库安全组
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: production
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
      annotations:
        k8s.aliyun.com/security-groups: "sg-bp1database"
        k8s.aliyun.com/pod-bindable: "true"
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
```

### 安全组规则示例

```bash
# 创建前端安全组规则
aliyun ecs AuthorizeSecurityGroup \
  --SecurityGroupId sg-bp1frontend \
  --IpProtocol tcp \
  --PortRange 80/80 \
  --SourceCidrIp 0.0.0.0/0 \
  --Description "Allow HTTP from anywhere"

# 创建后端安全组规则 (只允许前端访问)
aliyun ecs AuthorizeSecurityGroup \
  --SecurityGroupId sg-bp1backend \
  --IpProtocol tcp \
  --PortRange 8080/8080 \
  --SourceGroupId sg-bp1frontend \
  --Description "Allow from frontend security group"

# 创建数据库安全组规则 (只允许后端访问)
aliyun ecs AuthorizeSecurityGroup \
  --SecurityGroupId sg-bp1database \
  --IpProtocol tcp \
  --PortRange 3306/3306 \
  --SourceGroupId sg-bp1backend \
  --Description "Allow from backend security group"
```

## 5.4 企业安全最佳实践

### 安全配置清单

```yaml
# 安全配置检查清单
security_checklist:
  network_isolation:
    - name: "默认拒绝策略"
      status: "必须"
      description: "每个 Namespace 默认拒绝所有入站流量"
    
    - name: "最小权限原则"
      status: "必须"
      description: "只开放必要的端口和协议"
    
    - name: "Pod 安全组"
      status: "推荐"
      description: "敏感工作负载使用独立安全组"
  
  access_control:
    - name: "RBAC 配置"
      status: "必须"
      description: "限制网络资源的操作权限"
    
    - name: "Namespace 隔离"
      status: "必须"
      description: "不同环境使用不同 Namespace"
  
  monitoring:
    - name: "网络流量审计"
      status: "推荐"
      description: "启用流量日志记录"
    
    - name: "异常检测"
      status: "推荐"
      description: "配置网络异常告警"
```

### 企业级网络策略模板

```yaml
# 企业级 Namespace 网络隔离模板
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: enterprise-baseline
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # 允许同 Namespace 通信
  - from:
    - podSelector: {}
  # 允许 Ingress Controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
  # 允许监控系统
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
  egress:
  # 允许 DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # 允许同 Namespace 通信
  - to:
    - podSelector: {}
  # 允许访问阿里云内网服务
  - to:
    - ipBlock:
        cidr: 100.100.0.0/16
  # 允许访问指定外部 IP
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
    ports:
    - protocol: TCP
      port: 443
```

---

*第五章完 - 掌握了 Terway 高级特性与安全配置*

---

# 第六章 监控告警与故障排查

## 6.1 关键监控指标

### Terway 核心指标

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Terway 监控指标体系                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  IP 池指标:                                                                 │
│  ├─ terway_ip_pool_total          # IP 池总容量                            │
│  ├─ terway_ip_pool_allocated      # 已分配 IP 数                           │
│  ├─ terway_ip_pool_available      # 可用 IP 数                             │
│  └─ terway_ip_pool_utilization    # IP 池利用率 (%)                        │
│                                                                             │
│  ENI 指标:                                                                  │
│  ├─ terway_eni_total              # ENI 总数                               │
│  ├─ terway_eni_attached           # 已挂载 ENI 数                          │
│  ├─ terway_eni_ip_count           # ENI 辅助 IP 数                         │
│  └─ terway_eni_create_latency     # ENI 创建延迟 (ms)                      │
│                                                                             │
│  API 调用指标:                                                              │
│  ├─ terway_api_call_total         # API 调用总次数                         │
│  ├─ terway_api_call_errors        # API 调用失败次数                       │
│  ├─ terway_api_call_latency       # API 调用延迟 (ms)                      │
│  └─ terway_api_rate_limit_hits    # 触发限流次数                           │
│                                                                             │
│  Pod 网络指标:                                                              │
│  ├─ terway_pod_bindable_count     # 固定 IP Pod 数                         │
│  ├─ terway_pod_bindable_ips       # 已绑定固定 IP 数                       │
│  └─ terway_cni_operation_latency  # CNI 操作延迟 (ms)                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Prometheus 监控配置

```yaml
# ServiceMonitor 配置
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: terway-metrics
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: terway
  namespaceSelector:
    matchNames:
    - kube-system
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
---
# Terway 指标采集 Service
apiVersion: v1
kind: Service
metadata:
  name: terway-metrics
  namespace: kube-system
  labels:
    app: terway
spec:
  selector:
    app: terway
  ports:
  - name: metrics
    port: 9191
    targetPort: 9191
```

### 关键告警规则

```yaml
# Prometheus 告警规则
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: terway-alerts
  namespace: monitoring
spec:
  groups:
  - name: terway.rules
    interval: 30s
    rules:
    # IP 池耗尽预警
    - alert: TerwayIPPoolExhausted
      expr: |
        (terway_ip_pool_available / terway_ip_pool_total) < 0.1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Terway IP pool nearly exhausted"
        description: "Node {{ $labels.node }} IP pool available ratio is {{ $value | humanizePercentage }}"
    
    # IP 池利用率高
    - alert: TerwayIPPoolHighUtilization
      expr: |
        terway_ip_pool_utilization > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Terway IP pool high utilization"
        description: "Node {{ $labels.node }} IP pool utilization is {{ $value }}%"
    
    # ENI 创建延迟高
    - alert: TerwayENICreateLatencyHigh
      expr: |
        histogram_quantile(0.99, terway_eni_create_latency_bucket) > 5000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Terway ENI creation latency high"
        description: "ENI creation P99 latency is {{ $value }}ms"
    
    # API 调用失败率高
    - alert: TerwayAPIErrorRateHigh
      expr: |
        rate(terway_api_call_errors[5m]) / rate(terway_api_call_total[5m]) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Terway API error rate high"
        description: "API error rate is {{ $value | humanizePercentage }}"
    
    # CNI 操作延迟高
    - alert: TerwayCNILatencyHigh
      expr: |
        histogram_quantile(0.99, terway_cni_operation_latency_bucket) > 3000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Terway CNI operation latency high"
        description: "CNI operation P99 latency is {{ $value }}ms"
    
    # Terway Pod 异常
    - alert: TerwayPodNotReady
      expr: |
        kube_pod_status_ready{namespace="kube-system", pod=~"terway.*"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Terway pod not ready"
        description: "Pod {{ $labels.pod }} is not ready"
```

### Grafana 仪表板配置

```json
{
  "dashboard": {
    "title": "Terway Network Dashboard",
    "panels": [
      {
        "title": "IP Pool Utilization by Node",
        "type": "gauge",
        "targets": [{
          "expr": "terway_ip_pool_utilization",
          "legendFormat": "{{ node }}"
        }],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 60},
                {"color": "red", "value": 80}
              ]
            }
          }
        }
      },
      {
        "title": "IP Allocation Rate",
        "type": "graph",
        "targets": [{
          "expr": "rate(terway_ip_pool_allocated[5m])",
          "legendFormat": "{{ node }}"
        }]
      },
      {
        "title": "ENI Creation Latency P99",
        "type": "graph",
        "targets": [{
          "expr": "histogram_quantile(0.99, rate(terway_eni_create_latency_bucket[5m]))",
          "legendFormat": "P99 Latency"
        }]
      },
      {
        "title": "API Call Error Rate",
        "type": "graph",
        "targets": [{
          "expr": "rate(terway_api_call_errors[5m]) / rate(terway_api_call_total[5m]) * 100",
          "legendFormat": "Error Rate %"
        }]
      }
    ]
  }
}
```

## 6.2 常见故障诊断流程

### 故障诊断决策树

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Terway 故障诊断流程                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Pod 网络问题?                                                              │
│  │                                                                          │
│  ├─ Pod 无法获取 IP?                                                       │
│  │  ├─ 检查 Terway Pod 状态                                                │
│  │  │  kubectl get pods -n kube-system -l app=terway                       │
│  │  ├─ 检查 IP 池状态                                                      │
│  │  │  terway-cli show                                                     │
│  │  ├─ 检查 VSwitch IP 配额                                                │
│  │  │  阿里云控制台 > VPC > VSwitch                                        │
│  │  └─ 检查 ENI 配额                                                       │
│  │     阿里云控制台 > ECS > 配额                                           │
│  │                                                                          │
│  ├─ Pod 间无法通信?                                                        │
│  │  ├─ 同节点 Pod 通信                                                     │
│  │  │  ├─ 检查 veth 接口                                                   │
│  │  │  └─ 检查路由表                                                       │
│  │  ├─ 跨节点 Pod 通信                                                     │
│  │  │  ├─ 检查 VPC 路由                                                    │
│  │  │  ├─ 检查安全组规则                                                   │
│  │  │  └─ 检查 NetworkPolicy                                               │
│  │  └─ Pod 到 Service 通信                                                 │
│  │     ├─ 检查 kube-proxy 状态                                             │
│  │     └─ 检查 IPVS/iptables 规则                                          │
│  │                                                                          │
│  ├─ 网络延迟高?                                                            │
│  │  ├─ 检查跨可用区流量                                                    │
│  │  ├─ 检查 ENI 带宽限制                                                   │
│  │  └─ 检查网络策略开销                                                    │
│  │                                                                          │
│  └─ 网络丢包?                                                              │
│     ├─ 检查安全组连接跟踪                                                  │
│     ├─ 检查 conntrack 表                                                   │
│     └─ 检查网卡队列                                                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 常用诊断命令

```bash
# 1. 检查 Terway 组件状态
kubectl get pods -n kube-system -l app=terway -o wide
kubectl logs -n kube-system <terway-pod> -c terway --tail=100

# 2. 查看节点网络状态
kubectl exec -n kube-system <terway-pod> -c terway -- terway-cli show

# 输出示例:
# ┌──────────────────────────────────────────────────────────────┐
# │ Node: cn-hangzhou.10.0.1.10                                 │
# ├──────────────────────────────────────────────────────────────┤
# │ ENI Count: 3                                                 │
# │ IP Pool: Total=30, Allocated=25, Available=5                │
# │ Utilization: 83%                                             │
# ├──────────────────────────────────────────────────────────────┤
# │ ENI Details:                                                 │
# │   eni-bp1xxx (eth1): Primary=10.0.1.10, Secondary=10 IPs    │
# │   eni-bp2xxx (eth2): Primary=10.0.1.20, Secondary=10 IPs    │
# │   eni-bp3xxx (eth3): Primary=10.0.1.30, Secondary=10 IPs    │
# └──────────────────────────────────────────────────────────────┘

# 3. 检查 Pod 网络配置
kubectl exec <pod-name> -- ip addr
kubectl exec <pod-name> -- ip route
kubectl exec <pod-name> -- cat /etc/resolv.conf

# 4. 测试网络连通性
kubectl exec <pod-name> -- ping -c 4 <target-ip>
kubectl exec <pod-name> -- curl -v http://<service-name>:<port>
kubectl exec <pod-name> -- nslookup <service-name>

# 5. 检查 CNI 配置
kubectl exec -n kube-system <terway-pod> -c terway -- cat /etc/cni/net.d/10-terway.conf

# 6. 查看网络策略
kubectl get networkpolicies -A
kubectl describe networkpolicy <policy-name> -n <namespace>

# 7. 检查安全组
kubectl get pods <pod-name> -o jsonpath='{.metadata.annotations.k8s\.aliyun\.com/security-groups}'

# 8. 查看 IPVS 规则 (如果使用 IPVS 模式)
kubectl exec -n kube-system <kube-proxy-pod> -- ipvsadm -Ln
```

### 常见问题解决方案

| 问题 | 可能原因 | 解决方案 |
|------|----------|----------|
| Pod Pending (网络) | IP 池耗尽 | 扩容节点或调整 max_pool_size |
| Pod 无法获取 IP | ENI 配额不足 | 申请提升 ENI 配额 |
| 跨节点通信失败 | 安全组规则 | 检查并添加安全组规则 |
| Service 访问超时 | kube-proxy 异常 | 重启 kube-proxy 或检查 IPVS |
| DNS 解析失败 | CoreDNS 问题 | 检查 CoreDNS Pod 状态 |
| 网络延迟高 | 跨可用区流量 | 使用拓扑感知调度 |

## 6.3 性能瓶颈分析方法

### 网络性能测试

```bash
# 1. 使用 iperf3 测试网络带宽
# 服务端
kubectl run iperf-server --image=networkstatic/iperf3 --port=5201 -- -s

# 客户端测试
kubectl run iperf-client --rm -it --image=networkstatic/iperf3 -- \
  -c <iperf-server-ip> -t 30 -P 4

# 2. 使用 netperf 测试延迟
kubectl run netperf-server --image=networkstatic/netperf --port=12865 -- netserver

kubectl run netperf-client --rm -it --image=networkstatic/netperf -- \
  netperf -H <server-ip> -t TCP_RR -l 30

# 3. 使用 qperf 测试 RDMA (如果支持)
kubectl run qperf-server --image=mellanox/sockperf -- qperf

kubectl run qperf-client --rm -it --image=mellanox/sockperf -- \
  qperf <server-ip> tcp_bw tcp_lat

# 4. 测试 Pod 创建延迟
time kubectl run test-pod --image=busybox --rm -it --restart=Never -- echo "Pod ready"
```

### 性能分析工具

```bash
# 1. 查看网卡统计
kubectl exec -n kube-system <terway-pod> -c terway -- \
  cat /proc/net/dev

# 2. 查看 conntrack 状态
kubectl exec -n kube-system <kube-proxy-pod> -- \
  conntrack -S

# 3. 查看软中断分布
kubectl exec -n kube-system <terway-pod> -c terway -- \
  cat /proc/interrupts | grep -E "eth|eni"

# 4. 网络命名空间分析
kubectl exec -n kube-system <terway-pod> -c terway -- \
  ip netns list

# 5. 抓包分析
kubectl exec -n kube-system <terway-pod> -c terway -- \
  tcpdump -i any -nn -c 100 host <target-ip>
```

### 性能优化检查清单

```yaml
# 性能优化检查清单
performance_checklist:
  network_config:
    - item: "ENI 数量充足"
      check: "terway-cli show"
      target: "ENI 利用率 < 80%"
    
    - item: "IP 池预热充足"
      check: "检查 min_pool_size 配置"
      target: "min_pool_size >= 预期并发 Pod 数"
    
    - item: "跨可用区流量优化"
      check: "检查 Service 拓扑配置"
      target: "启用 topology-aware-hints"
  
  system_config:
    - item: "conntrack 表大小"
      check: "sysctl net.netfilter.nf_conntrack_max"
      target: ">= 1000000"
    
    - item: "网卡队列"
      check: "ethtool -l ethX"
      target: "队列数 = CPU 核数"
    
    - item: "中断亲和性"
      check: "cat /proc/interrupts"
      target: "均匀分布在各 CPU"
  
  application_config:
    - item: "连接池配置"
      check: "应用连接池大小"
      target: "根据负载调整"
    
    - item: "超时设置"
      check: "应用网络超时"
      target: "合理超时值"
```

---

*第六章完 - 掌握了 Terway 监控告警与故障排查*

---

# 第七章 性能优化与容量规划

## 7.1 eBPF 加速配置

### eBPF 加速原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          eBPF 网络加速原理                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  传统模式 (iptables/IPVS):                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Application                                                        │   │
│  │       ↓                                                              │   │
│  │  Socket Layer                                                        │   │
│  │       ↓                                                              │   │
│  │  TCP/IP Stack ───→ Netfilter ───→ iptables/IPVS ───→ 路由决策       │   │
│  │       ↓                              (多次遍历)                      │   │
│  │  Network Driver                                                      │   │
│  │       ↓                                                              │   │
│  │  NIC (网卡)                                                          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  eBPF 加速模式:                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Application                                                        │   │
│  │       ↓                                                              │   │
│  │  Socket Layer ◀──────────────────────────────────────┐              │   │
│  │       ↓                                               │              │   │
│  │  eBPF (TC/XDP) ─────→ 直接转发 (绕过 Netfilter) ──────┘              │   │
│  │       ↓                                                              │   │
│  │  Network Driver                                                      │   │
│  │       ↓                                                              │   │
│  │  NIC (网卡)                                                          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  性能提升:                                                                   │
│  ├─ 网络延迟降低 30-50%                                                    │
│  ├─ CPU 使用率降低 40-60%                                                  │
│  ├─ 吞吐量提升 20-40%                                                      │
│  └─ 连接跟踪效率提升                                                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 启用 eBPF 加速

```yaml
# ACK 集群启用 Terway + Cilium eBPF
# 方式1: 创建集群时选择

# 方式2: 升级现有集群组件
# 通过阿里云控制台: 集群 > 组件管理 > terway-eniip > 升级

# 验证 Cilium 组件状态
kubectl get pods -n kube-system -l k8s-app=cilium

# 检查 eBPF 功能
kubectl exec -n kube-system <cilium-pod> -- cilium status

# 输出示例:
# KVStore:                Ok   Disabled
# Kubernetes:             Ok   1.28 (v1.28.3) [linux/amd64]
# Kubernetes APIs:        ["cilium/v2::CiliumClusterwideNetworkPolicy", ...]
# KubeProxyReplacement:   True   [eth0 10.0.1.10]
# Host firewall:          Disabled
# CNI Chaining:           terway
# Cilium:                 Ok   1.14.4 (v1.14.4)
# NodeMonitor:            Listening for events on 8 CPUs with 64x4096 of shared memory
# Cilium health daemon:   Ok
# IPAM:                   IPv4: 5/254 allocated from 10.0.1.0/24
# BandwidthManager:       Disabled
# Host Routing:           Legacy
# Masquerading:           IPTables [IPv4: Enabled, IPv6: Disabled]
# Controller Status:      45/45 healthy
# Proxy Status:           OK, ip 10.0.1.10, 0 redirects active on ports 10000-20000
# Global Identity Range:  min 256, max 65535
# Hubble:                 Ok   Current/Max Flows: 4095/4095, Flows/s: 12.34
# Encryption:             Disabled
# Cluster health:         3/3 reachable   (2024-01-15T10:30:00Z)
```

### eBPF 高级配置

```yaml
# Cilium ConfigMap 优化配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
data:
  # 启用 kube-proxy 替代
  kube-proxy-replacement: "true"
  
  # 启用 DSR (Direct Server Return) 模式
  enable-node-port: "true"
  node-port-mode: "dsr"
  
  # 启用带宽管理
  enable-bandwidth-manager: "true"
  
  # 启用本地重定向优化
  enable-local-redirect-policy: "true"
  
  # 会话亲和性
  enable-session-affinity: "true"
  
  # Hubble 可观测性
  enable-hubble: "true"
  hubble-listen-address: ":4244"
  hubble-metrics-server: ":9965"
  hubble-metrics:
    - dns
    - drop
    - tcp
    - flow
    - icmp
    - http
```

## 7.2 ENI 预热与池化管理

### IP 池预热策略

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          IP 池预热策略                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  场景分析:                                                                   │
│                                                                             │
│  业务特点          │ min_pool_size │ max_pool_size │ 说明                   │
│  ─────────────────┼───────────────┼───────────────┼────────────────────────│
│  稳定负载          │     10        │     25        │ 标准配置               │
│  突发流量          │     30        │     80        │ 预留更多空闲 IP        │
│  批处理任务        │     50        │    150        │ 大量并发 Pod 创建      │
│  CI/CD 环境        │     20        │     60        │ 频繁创建销毁           │
│  Serverless        │     40        │    200        │ 冷启动优化             │
│                                                                             │
│  预热时机:                                                                   │
│  ├─ 节点启动时: 预热至 min_pool_size                                        │
│  ├─ IP 分配后: 如果可用 IP < min_pool_size，触发预热                        │
│  └─ 定时检查: 周期性检查并补充 IP 池                                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 动态调整 IP 池

```yaml
# 节点级别 IP 池配置
# 通过节点注解动态调整

# 高负载节点 - 增大 IP 池
kubectl annotate node <node-name> \
  k8s.aliyun.com/eni-max-pool-size=100 \
  k8s.aliyun.com/eni-min-pool-size=50 \
  --overwrite

# 低负载节点 - 减小 IP 池 (节省资源)
kubectl annotate node <node-name> \
  k8s.aliyun.com/eni-max-pool-size=20 \
  k8s.aliyun.com/eni-min-pool-size=5 \
  --overwrite

# 查看节点 IP 池配置
kubectl get node <node-name> -o jsonpath='{.metadata.annotations}' | \
  jq 'with_entries(select(.key | startswith("k8s.aliyun.com/eni")))'
```

### ENI 预挂载优化

```yaml
# 生产环境 ENI 预热配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx"]
      },
      
      // IP 池配置
      "max_pool_size": 80,
      "min_pool_size": 40,
      
      // ENI 预挂载配置
      "eni_cap_policy": "prefer_eni",
      "pre_attach_eni_count": 2,
      
      // IP 预热配置
      "ip_warmup": {
        "enable": true,
        "warmup_interval_seconds": 60,
        "warmup_batch_size": 10
      },
      
      // 资源回收配置
      "resource_gc": {
        "enable": true,
        "interval_seconds": 300,
        "idle_ip_ttl_seconds": 600,
        "orphan_eni_ttl_seconds": 1800
      }
    }
```

## 7.3 大规模集群优化方案

### 大规模集群挑战

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        大规模集群网络挑战                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  规模定义:                                                                   │
│  ├─ 中等规模: 100-500 节点, 5000-25000 Pods                                │
│  ├─ 大规模: 500-2000 节点, 25000-100000 Pods                               │
│  └─ 超大规模: 2000+ 节点, 100000+ Pods                                     │
│                                                                             │
│  主要挑战:                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  1. API 限流                                                        │   │
│  │     阿里云 ECS/VPC API 有 QPS 限制                                   │   │
│  │     解决: 批量操作、请求合并、限流配置                                │   │
│  │                                                                     │   │
│  │  2. IP 地址消耗                                                     │   │
│  │     大量 Pod 消耗大量 VPC IP                                         │   │
│  │     解决: 合理规划 VSwitch 网段、使用 Trunking 模式                  │   │
│  │                                                                     │   │
│  │  3. ENI 配额限制                                                    │   │
│  │     ECS 实例 ENI 数量有限                                           │   │
│  │     解决: 选择高配实例、申请配额提升                                 │   │
│  │                                                                     │   │
│  │  4. 控制平面压力                                                    │   │
│  │     大量网络事件增加 etcd 负载                                       │   │
│  │     解决: 优化 watch 机制、增加缓存                                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 大规模优化配置

```yaml
# 大规模集群 Terway 优化配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: eni-config
  namespace: kube-system
data:
  eni_conf: |
    {
      "version": "1",
      "security_group": "sg-bp1xxx",
      "service_cidr": "172.16.0.0/16",
      "vswitches": {
        "cn-hangzhou-h": ["vsw-bp1xxx", "vsw-bp2xxx", "vsw-bp3xxx"],
        "cn-hangzhou-i": ["vsw-bp4xxx", "vsw-bp5xxx", "vsw-bp6xxx"]
      },
      
      // 大规模 IP 池配置
      "max_pool_size": 100,
      "min_pool_size": 50,
      
      // API 限流优化
      "rate_limit": {
        "eni_create_qps": 10,
        "eni_delete_qps": 10,
        "ip_assign_qps": 50,
        "ip_unassign_qps": 50,
        "burst_size": 20
      },
      
      // 批量操作优化
      "batch_config": {
        "enable": true,
        "batch_size": 20,
        "batch_interval_ms": 100
      },
      
      // 缓存配置
      "cache_config": {
        "eni_cache_ttl_seconds": 300,
        "ip_cache_ttl_seconds": 60,
        "vpc_cache_ttl_seconds": 600
      },
      
      // 高可用配置
      "ha_config": {
        "leader_election": true,
        "lease_duration_seconds": 15,
        "renew_deadline_seconds": 10,
        "retry_period_seconds": 2
      }
    }
```

### 节点池分层策略

```yaml
# 大规模集群节点池分层

# 1. 系统节点池 (高可用)
apiVersion: v1
kind: NodePool
metadata:
  name: system-pool
spec:
  labels:
    node-role: system
  taints:
  - key: node-role
    value: system
    effect: NoSchedule
  annotations:
    k8s.aliyun.com/eni-max-pool-size: "30"
    k8s.aliyun.com/eni-min-pool-size: "15"

# 2. 业务节点池 (标准)
apiVersion: v1
kind: NodePool
metadata:
  name: business-pool
spec:
  labels:
    node-role: business
  annotations:
    k8s.aliyun.com/eni-max-pool-size: "80"
    k8s.aliyun.com/eni-min-pool-size: "40"

# 3. 弹性节点池 (突发)
apiVersion: v1
kind: NodePool
metadata:
  name: elastic-pool
spec:
  labels:
    node-role: elastic
  annotations:
    k8s.aliyun.com/eni-max-pool-size: "150"
    k8s.aliyun.com/eni-min-pool-size: "80"
```

## 7.4 容量规划最佳实践

### 容量规划公式

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          容量规划计算公式                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 单节点 Pod 容量计算:                                                    │
│     Pod容量 = (ENI数 - 1) × 每ENI辅助IP + 主ENI辅助IP                       │
│                                                                             │
│     示例 (ecs.g7.4xlarge):                                                  │
│     ENI数 = 8, 每ENI辅助IP = 30                                            │
│     Pod容量 = (8-1) × 30 + 30 = 240 Pods                                   │
│                                                                             │
│  2. 集群总 Pod 容量:                                                        │
│     集群Pod容量 = 节点数 × 单节点Pod容量 × 利用率系数(0.8)                   │
│                                                                             │
│     示例 (100 节点):                                                        │
│     集群Pod容量 = 100 × 240 × 0.8 = 19200 Pods                             │
│                                                                             │
│  3. VSwitch IP 需求:                                                        │
│     IP需求 = 集群Pod容量 × 1.3 (预留30%扩展)                                │
│                                                                             │
│     示例:                                                                   │
│     IP需求 = 19200 × 1.3 = 24960 IPs                                       │
│     推荐网段: /17 (32766 IPs) 或更大                                        │
│                                                                             │
│  4. ENI 配额需求:                                                           │
│     ENI配额 = 节点数 × 平均ENI数/节点 × 1.2                                 │
│                                                                             │
│     示例:                                                                   │
│     ENI配额 = 100 × 4 × 1.2 = 480 ENIs                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 容量规划模板

```yaml
# 生产环境容量规划模板
capacity_planning:
  cluster_info:
    name: "production-cluster"
    region: "cn-hangzhou"
    availability_zones: 3
  
  scale_requirements:
    target_pods: 20000
    peak_pods: 30000
    growth_rate_yearly: 50%
  
  node_config:
    instance_type: "ecs.g7.4xlarge"
    eni_per_node: 8
    ips_per_eni: 30
    pods_per_node: 240
  
  network_planning:
    vpc_cidr: "10.0.0.0/8"
    node_vswitches:
      - zone: "cn-hangzhou-h"
        cidr: "10.0.1.0/24"
        capacity: 254
      - zone: "cn-hangzhou-i"
        cidr: "10.0.2.0/24"
        capacity: 254
      - zone: "cn-hangzhou-j"
        cidr: "10.0.3.0/24"
        capacity: 254
    pod_vswitches:
      - zone: "cn-hangzhou-h"
        cidr: "10.0.16.0/20"
        capacity: 4094
      - zone: "cn-hangzhou-i"
        cidr: "10.0.32.0/20"
        capacity: 4094
      - zone: "cn-hangzhou-j"
        cidr: "10.0.48.0/20"
        capacity: 4094
    service_cidr: "172.16.0.0/16"
  
  calculations:
    required_nodes: 84  # 20000 / 240
    total_pod_ips: 25200  # 84 * 240 * 1.25
    total_eni_quota: 500  # 84 * 8 * 1.2 (申请配额)
```

---

*第七章完 - 掌握了 Terway 性能优化与容量规划*

---

# 第八章 生产环境最佳实践

## 8.1 网络模式选型指南

### 选型决策矩阵

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Terway 网络模式选型指南                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  决策维度        │ VPC路由  │ ENI独占  │ ENIIP    │ Trunking │             │
│  ────────────────┼──────────┼──────────┼──────────┼──────────┤             │
│  集群规模        │ <50节点  │ 任意     │ 推荐     │ >200节点 │             │
│  Pod密度需求     │ 低       │ 极低     │ 中高     │ 极高     │             │
│  网络性能        │ 高       │ 最高     │ 高       │ 高       │             │
│  Pod安全组       │ ✗        │ ✓        │ ✓        │ ✓        │             │
│  固定IP需求      │ ✗        │ ✓        │ ✓        │ ✓        │             │
│  IP成本         │ 低       │ 最高     │ 中       │ 最低     │             │
│  配置复杂度     │ 低       │ 中       │ 中       │ 高       │             │
│  运维复杂度     │ 低       │ 中       │ 中       │ 高       │             │
│                                                                             │
│  推荐场景:                                                                   │
│  ├─ VPC 路由: 开发测试、小规模生产、快速验证                                │
│  ├─ ENI 独占: 金融安全、高性能计算、极致隔离                                │
│  ├─ ENIIP:    标准生产环境 (推荐默认选择)                                   │
│  └─ Trunking: Serverless、超大规模、高密度部署                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 典型场景选型

```yaml
# 场景1: 互联网应用 (推荐 ENIIP)
scenario_internet_app:
  characteristics:
    - 无状态微服务为主
    - 需要弹性伸缩
    - 中等 Pod 密度
  recommendation:
    mode: "ENIIP"
    config:
      max_pool_size: 50
      min_pool_size: 20
      instance_type: "ecs.g7.2xlarge"

# 场景2: 金融核心系统 (推荐 ENI 独占)
scenario_financial:
  characteristics:
    - 强安全隔离需求
    - Pod 级别安全组
    - 低 Pod 密度
  recommendation:
    mode: "ENI"
    config:
      security_group_per_pod: true
      instance_type: "ecs.g7.4xlarge"

# 场景3: 大数据平台 (推荐 ENIIP + 大实例)
scenario_bigdata:
  characteristics:
    - 大量数据处理 Pod
    - 高网络带宽需求
    - 中等 Pod 密度
  recommendation:
    mode: "ENIIP"
    config:
      max_pool_size: 100
      min_pool_size: 50
      instance_type: "ecs.g7ne.8xlarge"

# 场景4: Serverless 平台 (推荐 Trunking)
scenario_serverless:
  characteristics:
    - 极高 Pod 密度
    - 快速创建销毁
    - 冷启动敏感
  recommendation:
    mode: "Trunking"
    config:
      max_pool_size: 200
      min_pool_size: 100
      instance_type: "ecs.ebmg7.32xlarge"
```

## 8.2 安全加固配置

### 网络安全加固清单

```yaml
# 生产环境网络安全加固清单
security_hardening:
  
  # 1. 网络隔离
  network_isolation:
    - action: "启用默认拒绝策略"
      priority: "必须"
      config: |
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
          name: default-deny-all
        spec:
          podSelector: {}
          policyTypes:
          - Ingress
          - Egress
    
    - action: "Namespace 级别隔离"
      priority: "必须"
      description: "不同环境使用不同 Namespace"
    
    - action: "Pod 安全组"
      priority: "推荐"
      description: "敏感工作负载使用独立安全组"
  
  # 2. 访问控制
  access_control:
    - action: "最小权限 NetworkPolicy"
      priority: "必须"
      description: "只开放必要的端口和协议"
    
    - action: "出站流量限制"
      priority: "推荐"
      description: "限制 Pod 访问外部网络"
    
    - action: "跨 Namespace 访问控制"
      priority: "推荐"
      description: "严格控制跨命名空间通信"
  
  # 3. 安全组配置
  security_groups:
    - action: "最小化安全组规则"
      priority: "必须"
      description: "删除不必要的入站规则"
    
    - action: "使用安全组引用"
      priority: "推荐"
      description: "使用源安全组而非 CIDR"
    
    - action: "定期审计安全组"
      priority: "推荐"
      description: "季度审计安全组规则"
```

### 安全加固配置示例

```yaml
# 生产环境 Namespace 完整安全配置
---
# 1. 默认拒绝所有流量
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
# 2. 允许 DNS 查询
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
---
# 3. 允许同 Namespace 通信
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector: {}
  egress:
  - to:
    - podSelector: {}
---
# 4. 允许 Ingress 入口流量
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress
  namespace: production
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/expose: "true"
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
---
# 5. 允许监控采集
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9090
    - protocol: TCP
      port: 8080
```

## 8.3 运维自动化工具链

### 运维脚本集

```bash
#!/bin/bash
# terway-ops.sh - Terway 运维工具集

# 1. 检查集群网络健康状态
check_network_health() {
    echo "=== Terway 组件状态 ==="
    kubectl get pods -n kube-system -l app=terway -o wide
    
    echo -e "\n=== IP 池状态汇总 ==="
    for pod in $(kubectl get pods -n kube-system -l app=terway -o jsonpath='{.items[*].metadata.name}'); do
        echo "--- Pod: $pod ---"
        kubectl exec -n kube-system $pod -c terway -- terway-cli show 2>/dev/null || echo "无法获取状态"
    done
    
    echo -e "\n=== 节点网络配置 ==="
    kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.annotations.k8s\.aliyun\.com/pod-bindable}{"\n"}{end}'
}

# 2. 检查 IP 池利用率
check_ip_utilization() {
    echo "=== IP 池利用率检查 ==="
    kubectl get pods -n kube-system -l app=terway -o jsonpath='{.items[*].metadata.name}' | \
    xargs -I {} kubectl exec -n kube-system {} -c terway -- terway-cli show 2>/dev/null | \
    grep -E "(Node|Pool|Utilization)"
}

# 3. 检查 NetworkPolicy
check_network_policies() {
    echo "=== NetworkPolicy 统计 ==="
    for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
        count=$(kubectl get networkpolicy -n $ns --no-headers 2>/dev/null | wc -l)
        if [ $count -gt 0 ]; then
            echo "$ns: $count 条策略"
        fi
    done
}

# 4. 网络连通性测试
test_connectivity() {
    local src_pod=$1
    local dst_target=$2
    
    echo "=== 测试 $src_pod -> $dst_target ==="
    kubectl exec $src_pod -- ping -c 3 $dst_target 2>/dev/null || \
    kubectl exec $src_pod -- curl -s -o /dev/null -w "%{http_code}" $dst_target 2>/dev/null || \
    echo "连接失败"
}

# 5. 导出网络诊断信息
export_diagnostics() {
    local output_dir="terway-diagnostics-$(date +%Y%m%d-%H%M%S)"
    mkdir -p $output_dir
    
    echo "导出诊断信息到 $output_dir ..."
    
    # Terway Pod 日志
    for pod in $(kubectl get pods -n kube-system -l app=terway -o jsonpath='{.items[*].metadata.name}'); do
        kubectl logs -n kube-system $pod -c terway --tail=1000 > "$output_dir/terway-$pod.log"
    done
    
    # 配置信息
    kubectl get configmap eni-config -n kube-system -o yaml > "$output_dir/eni-config.yaml"
    
    # NetworkPolicy
    kubectl get networkpolicy -A -o yaml > "$output_dir/network-policies.yaml"
    
    # 节点信息
    kubectl get nodes -o yaml > "$output_dir/nodes.yaml"
    
    echo "诊断信息已导出到 $output_dir"
}

# 主菜单
case "$1" in
    health)
        check_network_health
        ;;
    ip)
        check_ip_utilization
        ;;
    policy)
        check_network_policies
        ;;
    test)
        test_connectivity $2 $3
        ;;
    export)
        export_diagnostics
        ;;
    *)
        echo "用法: $0 {health|ip|policy|test|export}"
        echo "  health - 检查网络健康状态"
        echo "  ip     - 检查 IP 池利用率"
        echo "  policy - 检查 NetworkPolicy"
        echo "  test   - 测试网络连通性"
        echo "  export - 导出诊断信息"
        ;;
esac
```

### 自动化告警响应

```yaml
# 告警自动响应配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: terway-alert-playbook
  namespace: monitoring
data:
  playbook.yaml: |
    alerts:
      - name: "TerwayIPPoolExhausted"
        severity: critical
        auto_response:
          - action: "scale_node_pool"
            params:
              increment: 2
          - action: "notify"
            params:
              channel: "oncall"
              message: "Terway IP池耗尽，已自动扩容节点"
      
      - name: "TerwayIPPoolHighUtilization"
        severity: warning
        auto_response:
          - action: "adjust_pool_size"
            params:
              max_pool_size_delta: 20
          - action: "notify"
            params:
              channel: "ops"
              message: "Terway IP池利用率高，已调整池大小"
      
      - name: "TerwayPodNotReady"
        severity: critical
        auto_response:
          - action: "restart_pod"
            params:
              namespace: "kube-system"
              selector: "app=terway"
          - action: "notify"
            params:
              channel: "oncall"
              message: "Terway Pod异常，尝试自动重启"
```

## 8.4 生产案例实战

### 案例一: 电商大促网络优化

```yaml
# 电商大促场景网络优化方案
case_study_ecommerce:
  background:
    description: "双11大促，预计流量峰值10倍于日常"
    challenges:
      - "Pod 快速扩缩容"
      - "网络延迟敏感"
      - "高并发连接"
  
  solution:
    # 1. 提前扩容 IP 池
    pre_scaling:
      - action: "调整 IP 池大小"
        config:
          max_pool_size: 150
          min_pool_size: 80
      - action: "预创建节点"
        config:
          node_count: 50
          warm_up_time: "2h"
    
    # 2. 启用 eBPF 加速
    ebpf_acceleration:
      - action: "确认 Cilium 状态"
        check: "kubectl get pods -n kube-system -l k8s-app=cilium"
      - action: "验证 kube-proxy 替代"
        check: "cilium status | grep KubeProxyReplacement"
    
    # 3. 拓扑感知优化
    topology_optimization:
      - action: "启用拓扑感知 Service"
        config: |
          apiVersion: v1
          kind: Service
          metadata:
            annotations:
              service.kubernetes.io/topology-aware-hints: "Auto"
    
    # 4. 连接池优化
    connection_tuning:
      - action: "调整 conntrack"
        config:
          net.netfilter.nf_conntrack_max: 2000000
          net.netfilter.nf_conntrack_tcp_timeout_established: 1800
  
  results:
    - metric: "Pod 启动时间"
      before: "8-15s"
      after: "2-4s"
    - metric: "网络延迟 P99"
      before: "50ms"
      after: "15ms"
    - metric: "峰值 QPS"
      before: "50k"
      after: "200k"
```

### 案例二: 金融系统网络隔离

```yaml
# 金融系统网络隔离方案
case_study_financial:
  background:
    description: "银行核心系统，严格的安全合规要求"
    requirements:
      - "等保三级合规"
      - "业务隔离"
      - "审计追踪"
  
  solution:
    # 1. 网络模式选择
    network_mode:
      mode: "ENI 独占"
      reason: "Pod 级别安全组隔离"
    
    # 2. 安全组架构
    security_groups:
      - name: "sg-dmz"
        description: "DMZ 区域"
        rules:
          inbound:
            - port: 443
              source: "0.0.0.0/0"
      - name: "sg-app"
        description: "应用层"
        rules:
          inbound:
            - port: 8080
              source: "sg-dmz"
      - name: "sg-db"
        description: "数据层"
        rules:
          inbound:
            - port: 3306
              source: "sg-app"
    
    # 3. NetworkPolicy 配置
    network_policies:
      - name: "零信任策略"
        config: |
          # 默认拒绝所有
          apiVersion: networking.k8s.io/v1
          kind: NetworkPolicy
          metadata:
            name: default-deny
          spec:
            podSelector: {}
            policyTypes:
            - Ingress
            - Egress
    
    # 4. 审计日志
    audit_logging:
      - action: "启用网络流量日志"
        config:
          vpc_flow_log: true
          retention_days: 180
  
  compliance:
    - standard: "等保三级"
      status: "已通过"
    - standard: "PCI-DSS"
      status: "已通过"
```

### 案例三: 微服务架构网络治理

```yaml
# 微服务网络治理方案
case_study_microservices:
  background:
    description: "200+ 微服务的复杂调用链"
    challenges:
      - "服务间通信复杂"
      - "故障隔离困难"
      - "性能瓶颈定位难"
  
  solution:
    # 1. 服务网格集成
    service_mesh:
      type: "Istio + Terway"
      config:
        - "Terway 负责 L3/L4 网络"
        - "Istio 负责 L7 流量管理"
    
    # 2. 分层网络策略
    network_policies:
      - layer: "基础层"
        policies:
          - "默认拒绝"
          - "允许 DNS"
          - "允许监控"
      - layer: "业务层"
        policies:
          - "按服务组开放"
          - "调用链白名单"
    
    # 3. 可观测性
    observability:
      - tool: "Hubble"
        purpose: "网络流量可视化"
      - tool: "Prometheus"
        purpose: "网络指标监控"
      - tool: "Jaeger"
        purpose: "调用链追踪"
  
  results:
    - benefit: "故障定位时间"
      improvement: "从小时级到分钟级"
    - benefit: "网络问题发现"
      improvement: "主动发现率 95%"
```

---

*第八章完 - 掌握了 Terway 生产环境最佳实践*

---

## 附录 A: 常用命令速查表

```bash
# Terway 状态检查
kubectl get pods -n kube-system -l app=terway
kubectl exec -n kube-system <terway-pod> -c terway -- terway-cli show

# IP 池管理
kubectl annotate node <node> k8s.aliyun.com/eni-max-pool-size=100 --overwrite
kubectl annotate node <node> k8s.aliyun.com/eni-min-pool-size=50 --overwrite

# NetworkPolicy 管理
kubectl get networkpolicy -A
kubectl describe networkpolicy <name> -n <namespace>

# 网络诊断
kubectl exec <pod> -- ip addr
kubectl exec <pod> -- ip route
kubectl exec <pod> -- ping -c 3 <target>
kubectl exec <pod> -- curl -v http://<service>:<port>

# Cilium/eBPF 检查
kubectl get pods -n kube-system -l k8s-app=cilium
kubectl exec -n kube-system <cilium-pod> -- cilium status
```

## 附录 B: 配置模板索引

| 模板名称 | 适用场景 | 章节位置 |
|----------|----------|----------|
| ENIIP 基础配置 | 标准生产环境 | 2.3 节 |
| Trunking 配置 | 超高密度部署 | 2.4 节 |
| 多可用区配置 | 高可用部署 | 4.3 节 |
| 固定 IP 配置 | StatefulSet | 5.1 节 |
| 安全隔离配置 | 金融/合规场景 | 5.4 节 |
| 大规模优化配置 | 500+ 节点 | 7.3 节 |

## 附录 C: 故障排查索引

| 故障现象 | 可能原因 | 排查方法 | 章节位置 |
|----------|----------|----------|----------|
| Pod 无法获取 IP | IP 池耗尽 | terway-cli show | 6.2 节 |
| 跨节点通信失败 | 安全组/路由 | 检查安全组规则 | 6.2 节 |
| 网络延迟高 | 跨可用区 | 拓扑感知配置 | 7.1 节 |
| Pod 启动慢 | IP 预热不足 | 调整 min_pool_size | 7.2 节 |

---

**文档版本**: v2.0  
**更新日期**: 2026年1月  
**作者**: Kusheet Project  
**联系方式**: Allen Galler (allengaller@gmail.com)

---

*全文完 - Kubernetes Terway 从入门到实战*
